{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This .ipynb notebook was written by Rachel\n",
        "# Notebook used to initiate workspace, install dependencies into runtime and run shell script for training of RetGen model on Colab Pro's high RAM GPU.\n",
        "# Training stopped after 8h."
      ],
      "metadata": {
        "id": "QLcVeIPM-Sth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCH8aKnGaa2e"
      },
      "source": [
        "# Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA7QNBwpZx0k",
        "outputId": "95114eaf-0ddc-4907-8fbf-e12341ca28e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RetGen'...\n",
            "remote: Enumerating objects: 357, done.\u001b[K\n",
            "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
            "remote: Compressing objects: 100% (279/279), done.\u001b[K\n",
            "remote: Total 357 (delta 104), reused 304 (delta 65), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (357/357), 8.60 MiB | 15.05 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rachung2510/RetGen.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdJtd0gRagEj",
        "outputId": "4d9b3eba-2d71-414c-fbdb-6afcc4ab3b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/RetGen\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/rachung2510/RetGen\n",
            "   01c0815..ab2203b  main       -> origin/main\n",
            "Updating 01c0815..ab2203b\n",
            "Fast-forward\n",
            " run.sh | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/RetGen\n",
        "!git fetch\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lywjQRVhC9E"
      },
      "source": [
        "# Pip Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6UsPAp0hCfw",
        "outputId": "6cc67bfb-d4af-47ae-b3bf-40fb73ef0cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.7.0\n",
            "Collecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[K     |████████████████████████████████| 674 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 78.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==2.11.0) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.47 sentencepiece-0.1.96 transformers-2.11.0\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.4\n",
            "  Downloading botocore-1.24.4-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.4->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.4->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.4 botocore-1.24.4 jmespath-0.10.0 s3transfer-0.5.1 urllib3-1.26.8\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 82 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers==0.7.0\n",
        "!pip install transformers==2.11.0\n",
        "!pip install boto3\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQBxRTJ31CV_"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcTJQ2SPhYB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de5958c-5bbb-4572-c0f4-62f16df98c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8993, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 8993 (delta 26), reused 24 (delta 8), pack-reused 8929\u001b[K\n",
            "Receiving objects: 100% (8993/8993), 14.54 MiB | 12.78 MiB/s, done.\n",
            "Resolving deltas: 100% (6139/6139), done.\n",
            "/content/apex\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-00bxfoqc\n",
            "Created temporary directory: /tmp/pip-req-tracker-ahocvnu7\n",
            "Initialized build tracking at /tmp/pip-req-tracker-ahocvnu7\n",
            "Created build tracker: /tmp/pip-req-tracker-ahocvnu7\n",
            "Entered build tracker: /tmp/pip-req-tracker-ahocvnu7\n",
            "Created temporary directory: /tmp/pip-install-1ojuzcxr\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-2g7gqckv\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-ahocvnu7'\n",
            "    Running setup.py (path:/tmp/pip-req-build-2g7gqckv/setup.py) egg_info for package from file:///content/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-njdqs19p\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.10.0+cu111\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-njdqs19p/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-njdqs19p/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-njdqs19p/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-njdqs19p/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-njdqs19p/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-njdqs19p/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-2g7gqckv/setup.py:109: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-2g7gqckv has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-ahocvnu7'\n",
            "Created temporary directory: /tmp/pip-unpack-ik7moivz\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "  Created temporary directory: /tmp/pip-record-e7h3gxev\n",
            "    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-2g7gqckv/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-2g7gqckv/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-e7h3gxev/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.10.0+cu111\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-2g7gqckv/setup.py:109: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "    Cuda compilation tools, release 11.1, V11.1.105\n",
            "    Build cuda_11.1.TC455_06.29190527_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.7\n",
            "    creating build/lib.linux-x86_64-3.7/apex\n",
            "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.7/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.7/apex\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.7/apex/transformer\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n",
            "    creating build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.7/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.7/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.7/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.7/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.7/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/_data\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/amp\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n",
            "    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:381: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.7\n",
            "    creating build/temp.linux-x86_64-3.7/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_mp.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.7/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/welford.cu -o build/temp.linux-x86_64-3.7/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/syncbn.o build/temp.linux-x86_64-3.7/csrc/welford.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:157:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:178:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:179:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:180:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:202:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:270:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:271:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:272:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:273:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:274:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:275:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:313:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:332:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:333:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:353:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor rms_norm_gradient(at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:390:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:391:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:392:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:413:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:414:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:415:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:197:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:441:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:416:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.7/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:86: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "                                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "                                                               ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/mlp.o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'fused_dense_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.7/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:30:63: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                   ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:33:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:35:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:64:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "                                                                         ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:68:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias = at::empty({out_features}, input.type());\n",
            "                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:70:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:73:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:75:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:106:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:107:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:108:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:111:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:113:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:149:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "                                                                             ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:150:74: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "                                                                              ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:151:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "                                                              ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:152:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias2 = at::empty({out_features}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:153:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:154:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                            ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:157:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:159:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:276:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:194:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:278:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:176:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:281:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:282:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/fused_dense.o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.7/csrc/megatron\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-2g7gqckv/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-2g7gqckv/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    building 'scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-2g7gqckv/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-2g7gqckv/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-pX47U3/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.7/apex/_autocast_utils.py -> /usr/local/lib/python3.7/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.7/apex/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/log_util.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/standalone_bert.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/commons.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/parallel_state.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/_data/_batchsampler.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/_data/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/microbatches.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/enums.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/amp/grad_scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/amp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/amp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/permutation_lib.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck_module_test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fused_dense/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/distributed.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/multiproc.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.7/apex/parallel/LARC.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/cells.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/models.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.7/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/mlp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/mlp/mlp.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/data.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/output.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/base.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/db.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_amp_state.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_initialize.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/opt.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/handle.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/__version__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/rnn_compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/frontend.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/wrap.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.7/apex/amp/amp.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_mixed_precision_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/normalization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.7/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/log_util.py to log_util.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/commons.py to commons.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/_data/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/enums.py to enums.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/amp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/distributed.py to distributed.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/LARC.py to LARC.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/cells.py to cells.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/models.py to models.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/mlp.py to mlp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/data.py to data.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/output.py to output.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/base.py to base.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/db.py to db.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/compat.py to compat.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py to _initialize.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/opt.py to opt.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/handle.py to handle.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__version__.py to __version__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/utils.py to utils.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/scaler.py to scaler.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/frontend.py to frontend.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/wrap.py to wrap.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/amp.py to amp.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-37.pyc\n",
            "    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-37.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-e7h3gxev/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-ahocvnu7'\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL1XxCSjhfki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138f6e3d-d0ef-4ac5-8392-c9a498cafaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 30909, done.\u001b[K\n",
            "remote: Counting objects: 100% (541/541), done.\u001b[K\n",
            "remote: Compressing objects: 100% (335/335), done.\u001b[K\n",
            "remote: Total 30909 (delta 233), reused 403 (delta 196), pack-reused 30368\u001b[K\n",
            "Receiving objects: 100% (30909/30909), 21.22 MiB | 14.66 MiB/s, done.\n",
            "Resolving deltas: 100% (22931/22931), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.21.5)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.10.0+cu111)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (0.10.0+cu111)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (0.29.28)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (2019.12.20)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (1.15.0)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.3.7.tar.gz (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+5b87224) (4.62.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+5b87224) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+5b87224) (3.10.0.2)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+5b87224) (0.8.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+5b87224) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+5b87224) (3.7.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, bitarray\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=5c750c1249b8e7c1ae456ddf6fb2d4a9ca4936fb4596d08d8fa1aa6f830ce9c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for bitarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitarray: filename=bitarray-2.3.7-cp37-cp37m-linux_x86_64.whl size=173565 sha256=984824c77cf62109a550350705a2c140cb179fb3b5e062e30cf089e15ddb6414\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/65/b0/59479ecb406b1769a3126c2a633aad7365b956373d79724ef4\n",
            "Successfully built antlr4-python3-runtime bitarray\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.3.7 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmzuWrbR4p2r"
      },
      "source": [
        "# Preprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwZJX_lt8Flc",
        "outputId": "7d230b6f-71b3-47df-ffa5-73ead6992083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/RetGen\n",
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "100% 1042301/1042301 [00:00<00:00, 2761013.01B/s]\n",
            "100% 456318/456318 [00:00<00:00, 1458102.84B/s]\n",
            "100% 1479292/1479292 [40:28<00:00, 609.19it/s] \n"
          ]
        }
      ],
      "source": [
        "%cd /content/RetGen\n",
        "!python dialogpt/prepro.py --corpus \"/content/drive/MyDrive/Colab Notebooks/RetGen/data/arxiv_train.tsv\"  --max_seq_len 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVNjoNyYacXM"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3WUc1KaiIA",
        "outputId": "f9fcca69-c5eb-4109-9c59-f65434efa8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WsjvhXjakue",
        "outputId": "086e0c84-bebe-46cd-8c7b-35719da1cc75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RetGen\n",
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "Loading faiss with AVX2 support.\n",
            "Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "Loading faiss.\n",
            "Successfully loaded faiss.\n",
            "train batch size = 1, new train batch size (after gradient accumulation) = 1\n",
            "CUDA available? True\n",
            "Input Argument Information\n",
            "pretrained_model_cfg          bert-base-uncased\n",
            "encoder_model_type            ance_roberta\n",
            "pretrained_file               None\n",
            "model_file                    /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "projection_dim                0\n",
            "sequence_length               512\n",
            "adam_eps                      1e-08\n",
            "weight_decay                  0.0\n",
            "model_name_or_path            configs\n",
            "seed                          42\n",
            "max_seq_length                512\n",
            "skip_eval                     False\n",
            "init_checkpoint               /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "train_input_file              /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "eval_input_file               data/arxiv_test.txt\n",
            "continue_from                 0\n",
            "train_batch_size              1\n",
            "gradient_accumulation_steps   1\n",
            "eval_batch_size               1\n",
            "learning_rate                 1e-06\n",
            "num_optim_steps               16000\n",
            "valid_step                    200\n",
            "print_step                    100\n",
            "warmup_proportion             0.1\n",
            "warmup_steps                  1\n",
            "normalize_data                True\n",
            "lr_schedule                   noam\n",
            "loss_scale                    0\n",
            "no_token_id                   False\n",
            "output_dir                    /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "log_dir                       None\n",
            "pbar                          False\n",
            "set_type_embedding_to_zero    False\n",
            "reverse                       False\n",
            "avg_by_prob                   False\n",
            "rl_method                     simple\n",
            "file_suffix                   \n",
            "dropout                       0.1\n",
            "config                        None\n",
            "no_cuda                       False\n",
            "local_rank                    -1\n",
            "fp16                          False\n",
            "fp16_opt_level                O1\n",
            "ctx_file                      /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "shard_id                      0\n",
            "num_shards                    100\n",
            "batch_size                    8\n",
            "n_docs                        2\n",
            "validation_workers            16\n",
            "index_buffer                  50000\n",
            "hnsw_index                    False\n",
            "encoding                      True\n",
            "shard_folder                  False\n",
            "log_batch_step                100\n",
            "eval_on_each                  False\n",
            "retriever_master_rank         False\n",
            "r_only                        False\n",
            "g_only                        False\n",
            "ret_correction                False\n",
            "change_id_over_card           True\n",
            "load_trained_model            True\n",
            "do_lower_case                 True\n",
            "device                        cuda\n",
            "n_gpu                         1\n",
            "loading vocabulary file configs/vocab.json\n",
            "loading merges file configs/merges.txt\n",
            "Initialized host 7d361dc88840 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n",
            "PyTorch version 1.10.0+cu111 available.\n",
            "TensorFlow version 2.8.0 available.\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpp65tm6bi\n",
            "Downloading: 100% 433/433 [00:00<00:00, 439kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpit1wqin7\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 69.2MB/s]\n",
            "storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpatedhc2z\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 321kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "Encoder vector_size=768\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "loading finetuned model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "Number of parameter = 406286336\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 5.013 Val ppl 220.141 Val reward 0.009\n",
            "g step:1,eval_loss:5.01340984916687,eval_ppl:220.14120070934297,eval_reward:0.009269939836580306\n",
            "current learning rate: 1e-06\n",
            "/content/RetGen/dialogpt/lsp_model/optim.py:160: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "Epoch:1, Retrival step:2,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.305,reward:0.539,dot_prod:89.509\n",
            "Epoch:1, Retrival step:3,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.306,reward:0.284,dot_prod:-73.973\n",
            "Epoch:1, Retrival step:4,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.228,dot_prod:-33.957\n",
            "Epoch:1, Retrival step:5,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.179,dot_prod:-6.975\n",
            "Epoch:1, Retrival step:6,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.329,reward:0.146,dot_prod:10.904\n",
            "Epoch:1, Retrival step:7,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.127,dot_prod:19.737\n",
            "Epoch:1, Retrival step:8,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.116,dot_prod:28.669\n",
            "Epoch:1, Retrival step:9,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.328,reward:0.106,dot_prod:35.393\n",
            "Epoch:1, Retrival step:10,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.099,dot_prod:27.072\n",
            "Epoch:1, Retrival step:11,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.308,reward:0.091,dot_prod:22.379\n",
            "Epoch:1, Retrival step:12,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.084,dot_prod:18.112\n",
            "Epoch:1, Retrival step:13,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.081,dot_prod:7.543\n",
            "Epoch:1, Retrival step:14,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.080,dot_prod:-6.695\n",
            "Epoch:1, Retrival step:15,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.078,dot_prod:-12.931\n",
            "Epoch:1, Retrival step:16,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.076,dot_prod:-23.681\n",
            "Epoch:1, Retrival step:17,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.318,reward:0.073,dot_prod:-35.829\n",
            "Epoch:1, Retrival step:18,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.244,reward:0.073,dot_prod:-34.652\n",
            "Epoch:1, Retrival step:19,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.071,dot_prod:-32.992\n",
            "Epoch:1, Retrival step:20,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.068,dot_prod:-32.923\n",
            "Epoch:1, Retrival step:21,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.065,dot_prod:-33.080\n",
            "Epoch:1, Retrival step:22,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.296,reward:0.064,dot_prod:-31.662\n",
            "Epoch:1, Retrival step:23,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.281,reward:0.062,dot_prod:-30.016\n",
            "Epoch:1, Retrival step:24,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.060,dot_prod:-30.555\n",
            "Epoch:1, Retrival step:25,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.058,dot_prod:-28.205\n",
            "Epoch:1, Retrival step:26,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.056,dot_prod:-24.596\n",
            "Epoch:1, Retrival step:27,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.055,dot_prod:-21.087\n",
            "Epoch:1, Retrival step:28,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.054,dot_prod:-16.823\n",
            "Epoch:1, Retrival step:29,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.288,reward:0.052,dot_prod:-12.526\n",
            "Epoch:1, Retrival step:30,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.051,dot_prod:-8.275\n",
            "Epoch:1, Retrival step:31,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.050,dot_prod:-5.302\n",
            "Epoch:1, Retrival step:32,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.049,dot_prod:-2.150\n",
            "Epoch:1, Retrival step:33,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.048,dot_prod:-0.863\n",
            "Epoch:1, Retrival step:34,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.296,reward:0.046,dot_prod:-0.566\n",
            "Epoch:1, Retrival step:35,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.293,reward:0.045,dot_prod:0.333\n",
            "Epoch:1, Retrival step:36,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.045,dot_prod:1.270\n",
            "Epoch:1, Retrival step:37,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.044,dot_prod:1.970\n",
            "Epoch:1, Retrival step:38,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.273,reward:0.043,dot_prod:2.628\n",
            "Epoch:1, Retrival step:39,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.043,dot_prod:3.707\n",
            "Epoch:1, Retrival step:40,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.042,dot_prod:4.108\n",
            "Epoch:1, Retrival step:41,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.318,reward:0.042,dot_prod:5.486\n",
            "Epoch:1, Retrival step:42,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.041,dot_prod:7.803\n",
            "Epoch:1, Retrival step:43,loss:-0.000,ppl:0.000,ret_time:0.091,gen_time:0.297,reward:0.040,dot_prod:10.002\n",
            "Epoch:1, Retrival step:44,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.039,dot_prod:11.564\n",
            "Epoch:1, Retrival step:45,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.314,reward:0.039,dot_prod:12.425\n",
            "Epoch:1, Retrival step:46,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.038,dot_prod:12.866\n",
            "Epoch:1, Retrival step:47,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.038,dot_prod:13.071\n",
            "Epoch:1, Retrival step:48,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.037,dot_prod:11.914\n",
            "Epoch:1, Retrival step:49,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.037,dot_prod:10.481\n",
            "Epoch:1, Retrival step:50,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.276,reward:0.036,dot_prod:9.672\n",
            "Epoch:1, Retrival step:51,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.279,reward:0.036,dot_prod:8.631\n",
            "Epoch:1, Retrival step:52,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.326,reward:0.036,dot_prod:8.573\n",
            "Epoch:1, Retrival step:53,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.035,dot_prod:9.466\n",
            "Epoch:1, Retrival step:54,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.035,dot_prod:10.791\n",
            "Epoch:1, Retrival step:55,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.258,reward:0.034,dot_prod:12.298\n",
            "Epoch:1, Retrival step:56,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.034,dot_prod:13.239\n",
            "Epoch:1, Retrival step:57,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.034,dot_prod:14.474\n",
            "Epoch:1, Retrival step:58,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.034,dot_prod:14.744\n",
            "Epoch:1, Retrival step:59,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.033,dot_prod:16.051\n",
            "Epoch:1, Retrival step:60,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.033,dot_prod:18.527\n",
            "Epoch:1, Retrival step:61,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.276,reward:0.033,dot_prod:20.775\n",
            "Epoch:1, Retrival step:62,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.032,dot_prod:23.294\n",
            "Epoch:1, Retrival step:63,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.032,dot_prod:25.903\n",
            "Epoch:1, Retrival step:64,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.032,dot_prod:28.031\n",
            "Epoch:1, Retrival step:65,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.274,reward:0.031,dot_prod:30.589\n",
            "Epoch:1, Retrival step:66,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.338,reward:0.031,dot_prod:33.044\n",
            "Epoch:1, Retrival step:67,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.295,reward:0.031,dot_prod:35.563\n",
            "Epoch:1, Retrival step:68,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.283,reward:0.030,dot_prod:37.867\n",
            "Epoch:1, Retrival step:69,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.030,dot_prod:39.986\n",
            "Epoch:1, Retrival step:70,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.030,dot_prod:42.140\n",
            "Epoch:1, Retrival step:71,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.029,dot_prod:44.633\n",
            "Epoch:1, Retrival step:72,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.029,dot_prod:46.722\n",
            "Epoch:1, Retrival step:73,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.029,dot_prod:49.027\n",
            "Epoch:1, Retrival step:74,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.028,dot_prod:51.120\n",
            "Epoch:1, Retrival step:75,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.028,dot_prod:53.199\n",
            "Epoch:1, Retrival step:76,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.028,dot_prod:54.834\n",
            "Epoch:1, Retrival step:77,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.028,dot_prod:56.533\n",
            "Epoch:1, Retrival step:78,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.027,dot_prod:57.800\n",
            "Epoch:1, Retrival step:79,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.027,dot_prod:59.115\n",
            "Epoch:1, Retrival step:80,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.325,reward:0.027,dot_prod:60.412\n",
            "Epoch:1, Retrival step:81,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.294,reward:0.027,dot_prod:61.652\n",
            "Epoch:1, Retrival step:82,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.271,reward:0.027,dot_prod:62.706\n",
            "Epoch:1, Retrival step:83,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.027,dot_prod:63.972\n",
            "Epoch:1, Retrival step:84,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.280,reward:0.027,dot_prod:65.085\n",
            "Epoch:1, Retrival step:85,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.027,dot_prod:65.758\n",
            "Epoch:1, Retrival step:86,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.286,reward:0.027,dot_prod:66.746\n",
            "Epoch:1, Retrival step:87,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.027,dot_prod:67.727\n",
            "Epoch:1, Retrival step:88,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.027,dot_prod:68.611\n",
            "Epoch:1, Retrival step:89,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.026,dot_prod:69.300\n",
            "Epoch:1, Retrival step:90,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.026,dot_prod:70.077\n",
            "Epoch:1, Retrival step:91,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.026,dot_prod:70.850\n",
            "Epoch:1, Retrival step:92,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.026,dot_prod:71.576\n",
            "Epoch:1, Retrival step:93,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.026,dot_prod:72.347\n",
            "Epoch:1, Retrival step:94,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.283,reward:0.026,dot_prod:73.114\n",
            "Epoch:1, Retrival step:95,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.026,dot_prod:73.892\n",
            "Epoch:1, Retrival step:96,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.025,dot_prod:74.470\n",
            "Epoch:1, Retrival step:97,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.025,dot_prod:75.242\n",
            "Epoch:1, Retrival step:98,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.025,dot_prod:75.795\n",
            "Epoch:1, Retrival step:99,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.285,reward:0.025,dot_prod:76.206\n",
            "Generation step:100,loss:5.275,ppl:319.909,ret_time:0.080,gen_time:0.209\n",
            "Epoch:1, Retrival step:100,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.265,reward:0.025,dot_prod:76.605\n",
            "Epoch:1, Retrival step:101,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.025,dot_prod:76.906\n",
            "Epoch:1, Retrival step:102,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.025,dot_prod:77.425\n",
            "Epoch:1, Retrival step:103,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.025,dot_prod:77.969\n",
            "Epoch:1, Retrival step:104,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.025,dot_prod:78.635\n",
            "Epoch:1, Retrival step:105,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.329,reward:0.024,dot_prod:79.197\n",
            "Epoch:1, Retrival step:106,loss:-0.000,ppl:0.000,ret_time:0.091,gen_time:0.310,reward:0.024,dot_prod:79.491\n",
            "Epoch:1, Retrival step:107,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.024,dot_prod:79.980\n",
            "Epoch:1, Retrival step:108,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.024,dot_prod:80.304\n",
            "Epoch:1, Retrival step:109,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.310,reward:0.024,dot_prod:80.812\n",
            "Epoch:1, Retrival step:110,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.024,dot_prod:81.210\n",
            "Epoch:1, Retrival step:111,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.024,dot_prod:81.494\n",
            "Epoch:1, Retrival step:112,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.024,dot_prod:81.731\n",
            "Epoch:1, Retrival step:113,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.024,dot_prod:82.175\n",
            "Epoch:1, Retrival step:114,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.023,dot_prod:82.463\n",
            "Epoch:1, Retrival step:115,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.023,dot_prod:82.723\n",
            "Epoch:1, Retrival step:116,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.278,reward:0.023,dot_prod:83.078\n",
            "Epoch:1, Retrival step:117,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.287,reward:0.023,dot_prod:83.325\n",
            "Epoch:1, Retrival step:118,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.023,dot_prod:83.330\n",
            "Epoch:1, Retrival step:119,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.023,dot_prod:83.649\n",
            "Epoch:1, Retrival step:120,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.023,dot_prod:83.529\n",
            "Epoch:1, Retrival step:121,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.023,dot_prod:83.639\n",
            "Epoch:1, Retrival step:122,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.296,reward:0.023,dot_prod:83.689\n",
            "Epoch:1, Retrival step:123,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.023,dot_prod:83.922\n",
            "Epoch:1, Retrival step:124,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.023,dot_prod:84.104\n",
            "Epoch:1, Retrival step:125,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.323,reward:0.023,dot_prod:84.072\n",
            "Epoch:1, Retrival step:126,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.277,reward:0.023,dot_prod:83.942\n",
            "Epoch:1, Retrival step:127,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.023,dot_prod:84.067\n",
            "Epoch:1, Retrival step:128,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.023,dot_prod:84.095\n",
            "Epoch:1, Retrival step:129,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.022,dot_prod:84.092\n",
            "Epoch:1, Retrival step:130,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.022,dot_prod:84.133\n",
            "Epoch:1, Retrival step:131,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.022,dot_prod:84.325\n",
            "Epoch:1, Retrival step:132,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.022,dot_prod:84.338\n",
            "Epoch:1, Retrival step:133,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.022,dot_prod:84.438\n",
            "Epoch:1, Retrival step:134,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.022,dot_prod:84.607\n",
            "Epoch:1, Retrival step:135,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.022,dot_prod:84.594\n",
            "Epoch:1, Retrival step:136,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.305,reward:0.022,dot_prod:84.855\n",
            "Epoch:1, Retrival step:137,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.022,dot_prod:85.050\n",
            "Epoch:1, Retrival step:138,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.022,dot_prod:85.062\n",
            "Epoch:1, Retrival step:139,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.022,dot_prod:85.123\n",
            "Epoch:1, Retrival step:140,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.022,dot_prod:85.187\n",
            "Epoch:1, Retrival step:141,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.022,dot_prod:85.393\n",
            "Epoch:1, Retrival step:142,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.022,dot_prod:85.325\n",
            "Epoch:1, Retrival step:143,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.022,dot_prod:85.545\n",
            "Epoch:1, Retrival step:144,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.336,reward:0.022,dot_prod:85.635\n",
            "Epoch:1, Retrival step:145,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.022,dot_prod:85.711\n",
            "Epoch:1, Retrival step:146,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.022,dot_prod:85.978\n",
            "Epoch:1, Retrival step:147,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.022,dot_prod:86.070\n",
            "Epoch:1, Retrival step:148,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.021,dot_prod:86.224\n",
            "Epoch:1, Retrival step:149,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.021,dot_prod:86.393\n",
            "Epoch:1, Retrival step:150,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.303,reward:0.021,dot_prod:86.552\n",
            "Epoch:1, Retrival step:151,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.021,dot_prod:86.807\n",
            "Epoch:1, Retrival step:152,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.021,dot_prod:86.937\n",
            "Epoch:1, Retrival step:153,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.021,dot_prod:87.052\n",
            "Epoch:1, Retrival step:154,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.276,reward:0.021,dot_prod:87.186\n",
            "Epoch:1, Retrival step:155,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.244,reward:0.021,dot_prod:87.479\n",
            "Epoch:1, Retrival step:156,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.021,dot_prod:87.743\n",
            "Epoch:1, Retrival step:157,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.021,dot_prod:87.913\n",
            "Epoch:1, Retrival step:158,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.021,dot_prod:88.073\n",
            "Epoch:1, Retrival step:159,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.021,dot_prod:88.165\n",
            "Epoch:1, Retrival step:160,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.021,dot_prod:88.324\n",
            "Epoch:1, Retrival step:161,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.021,dot_prod:88.492\n",
            "Epoch:1, Retrival step:162,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.021,dot_prod:88.630\n",
            "Epoch:1, Retrival step:163,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.284,reward:0.021,dot_prod:88.745\n",
            "Epoch:1, Retrival step:164,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.021,dot_prod:88.937\n",
            "Epoch:1, Retrival step:165,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.286,reward:0.021,dot_prod:88.996\n",
            "Epoch:1, Retrival step:166,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.021,dot_prod:89.077\n",
            "Epoch:1, Retrival step:167,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.021,dot_prod:89.284\n",
            "Epoch:1, Retrival step:168,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.291,reward:0.021,dot_prod:89.306\n",
            "Epoch:1, Retrival step:169,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.021,dot_prod:89.486\n",
            "Epoch:1, Retrival step:170,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.021,dot_prod:89.532\n",
            "Epoch:1, Retrival step:171,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.021,dot_prod:89.661\n",
            "Epoch:1, Retrival step:172,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.270,reward:0.021,dot_prod:89.770\n",
            "Epoch:1, Retrival step:173,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.287,reward:0.021,dot_prod:89.763\n",
            "Epoch:1, Retrival step:174,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.021,dot_prod:89.897\n",
            "Epoch:1, Retrival step:175,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.021,dot_prod:89.969\n",
            "Epoch:1, Retrival step:176,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.285,reward:0.021,dot_prod:90.020\n",
            "Epoch:1, Retrival step:177,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.021,dot_prod:90.104\n",
            "Epoch:1, Retrival step:178,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.021,dot_prod:90.087\n",
            "Epoch:1, Retrival step:179,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.303,reward:0.021,dot_prod:90.087\n",
            "Epoch:1, Retrival step:180,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.309,reward:0.021,dot_prod:90.158\n",
            "Epoch:1, Retrival step:181,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.021,dot_prod:90.119\n",
            "Epoch:1, Retrival step:182,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.020,dot_prod:90.299\n",
            "Epoch:1, Retrival step:183,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:90.329\n",
            "Epoch:1, Retrival step:184,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:90.541\n",
            "Epoch:1, Retrival step:185,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.021,dot_prod:90.607\n",
            "Epoch:1, Retrival step:186,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.327,reward:0.021,dot_prod:90.770\n",
            "Epoch:1, Retrival step:187,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.316,reward:0.021,dot_prod:90.942\n",
            "Epoch:1, Retrival step:188,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.296,reward:0.021,dot_prod:90.972\n",
            "Epoch:1, Retrival step:189,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.021,dot_prod:91.099\n",
            "Epoch:1, Retrival step:190,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.021,dot_prod:91.206\n",
            "Epoch:1, Retrival step:191,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.021,dot_prod:91.355\n",
            "Epoch:1, Retrival step:192,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.336,reward:0.021,dot_prod:91.464\n",
            "Epoch:1, Retrival step:193,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.021,dot_prod:91.578\n",
            "Epoch:1, Retrival step:194,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.021,dot_prod:91.761\n",
            "Epoch:1, Retrival step:195,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.021,dot_prod:91.878\n",
            "Epoch:1, Retrival step:196,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.021,dot_prod:92.072\n",
            "Epoch:1, Retrival step:197,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.278,reward:0.021,dot_prod:92.202\n",
            "Epoch:1, Retrival step:198,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.282,reward:0.020,dot_prod:92.370\n",
            "Epoch:1, Retrival step:199,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:92.444\n",
            "Generation step:200,loss:4.981,ppl:236.089,ret_time:0.081,gen_time:0.275\n",
            "Epoch:1, Retrival step:200,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:92.599\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.608 Val ppl 127.566 Val reward 0.013\n",
            "r step:201,eval_loss:4.607913114070892,eval_ppl:127.56585555791855,eval_reward:0.01303249153937213\n",
            "current learning rate: 2.21525376565105e-05\n",
            "Epoch:1, Retrival step:201,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:92.650\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.558 Val ppl 121.779 Val reward 0.014\n",
            "g step:201,eval_loss:4.557610164880753,eval_ppl:121.77867392778397,eval_reward:0.013624595048371702\n",
            "current learning rate: 2.209708691207961e-05\n",
            "Epoch:1, Retrival step:202,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.290,reward:0.020,dot_prod:92.834\n",
            "Epoch:1, Retrival step:203,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.020,dot_prod:93.010\n",
            "Epoch:1, Retrival step:204,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:93.071\n",
            "Epoch:1, Retrival step:205,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:93.188\n",
            "Epoch:1, Retrival step:206,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.021,dot_prod:93.235\n",
            "Epoch:1, Retrival step:207,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.020,dot_prod:93.382\n",
            "Epoch:1, Retrival step:208,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.268,reward:0.020,dot_prod:93.463\n",
            "Epoch:1, Retrival step:209,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.020,dot_prod:93.605\n",
            "Epoch:1, Retrival step:210,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:93.707\n",
            "Epoch:1, Retrival step:211,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:93.803\n",
            "Epoch:1, Retrival step:212,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.020,dot_prod:93.940\n",
            "Epoch:1, Retrival step:213,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:93.932\n",
            "Epoch:1, Retrival step:214,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:94.060\n",
            "Epoch:1, Retrival step:215,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:94.119\n",
            "Epoch:1, Retrival step:216,loss:-0.000,ppl:0.000,ret_time:0.079,gen_time:0.254,reward:0.020,dot_prod:94.188\n",
            "Epoch:1, Retrival step:217,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:94.283\n",
            "Epoch:1, Retrival step:218,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.020,dot_prod:94.346\n",
            "Epoch:1, Retrival step:219,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:94.401\n",
            "Epoch:1, Retrival step:220,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.020,dot_prod:94.485\n",
            "Epoch:1, Retrival step:221,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:94.549\n",
            "Epoch:1, Retrival step:222,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.294,reward:0.020,dot_prod:94.664\n",
            "Epoch:1, Retrival step:223,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.317,reward:0.020,dot_prod:94.705\n",
            "Epoch:1, Retrival step:224,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.271,reward:0.020,dot_prod:94.837\n",
            "Epoch:1, Retrival step:225,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:94.907\n",
            "Epoch:1, Retrival step:226,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.020,dot_prod:95.033\n",
            "Epoch:1, Retrival step:227,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.020,dot_prod:95.267\n",
            "Epoch:1, Retrival step:228,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.020,dot_prod:95.329\n",
            "Epoch:1, Retrival step:229,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.020,dot_prod:95.357\n",
            "Epoch:1, Retrival step:230,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.020,dot_prod:95.427\n",
            "Epoch:1, Retrival step:231,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.020,dot_prod:95.516\n",
            "Epoch:1, Retrival step:232,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:95.572\n",
            "Epoch:1, Retrival step:233,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.335,reward:0.020,dot_prod:95.720\n",
            "Epoch:1, Retrival step:234,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.020,dot_prod:95.745\n",
            "Epoch:1, Retrival step:235,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:95.789\n",
            "Epoch:1, Retrival step:236,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:95.846\n",
            "Epoch:1, Retrival step:237,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.020,dot_prod:95.957\n",
            "Epoch:1, Retrival step:238,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.020,dot_prod:96.015\n",
            "Epoch:1, Retrival step:239,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:96.011\n",
            "Epoch:1, Retrival step:240,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.020,dot_prod:96.020\n",
            "Epoch:1, Retrival step:241,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.020,dot_prod:96.125\n",
            "Epoch:1, Retrival step:242,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:96.209\n",
            "Epoch:1, Retrival step:243,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.020,dot_prod:96.226\n",
            "Epoch:1, Retrival step:244,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.296,reward:0.020,dot_prod:96.217\n",
            "Epoch:1, Retrival step:245,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.020,dot_prod:96.302\n",
            "Epoch:1, Retrival step:246,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.020,dot_prod:96.301\n",
            "Epoch:1, Retrival step:247,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.020,dot_prod:96.380\n",
            "Epoch:1, Retrival step:248,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.020,dot_prod:96.420\n",
            "Epoch:1, Retrival step:249,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:96.460\n",
            "Epoch:1, Retrival step:250,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.261,reward:0.020,dot_prod:96.567\n",
            "Epoch:1, Retrival step:251,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:96.706\n",
            "Epoch:1, Retrival step:252,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.020,dot_prod:96.752\n",
            "Epoch:1, Retrival step:253,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.020,dot_prod:96.805\n",
            "Epoch:1, Retrival step:254,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:96.880\n",
            "Epoch:1, Retrival step:255,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:96.938\n",
            "Epoch:1, Retrival step:256,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.289,reward:0.020,dot_prod:96.939\n",
            "Epoch:1, Retrival step:257,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:96.939\n",
            "Epoch:1, Retrival step:258,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.020,dot_prod:96.980\n",
            "Epoch:1, Retrival step:259,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.317,reward:0.020,dot_prod:97.072\n",
            "Epoch:1, Retrival step:260,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.020,dot_prod:97.074\n",
            "Epoch:1, Retrival step:261,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.307,reward:0.020,dot_prod:97.143\n",
            "Epoch:1, Retrival step:262,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:97.259\n",
            "Epoch:1, Retrival step:263,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:97.325\n",
            "Epoch:1, Retrival step:264,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.279,reward:0.020,dot_prod:97.451\n",
            "Epoch:1, Retrival step:265,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.020,dot_prod:97.597\n",
            "Epoch:1, Retrival step:266,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.020,dot_prod:97.646\n",
            "Epoch:1, Retrival step:267,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:97.755\n",
            "Epoch:1, Retrival step:268,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.337,reward:0.020,dot_prod:97.872\n",
            "Epoch:1, Retrival step:269,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.324,reward:0.020,dot_prod:97.985\n",
            "Epoch:1, Retrival step:270,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.020,dot_prod:98.033\n",
            "Epoch:1, Retrival step:271,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.020,dot_prod:98.127\n",
            "Epoch:1, Retrival step:272,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.020,dot_prod:98.180\n",
            "Epoch:1, Retrival step:273,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.020,dot_prod:98.320\n",
            "Epoch:1, Retrival step:274,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.020,dot_prod:98.302\n",
            "Epoch:1, Retrival step:275,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.305,reward:0.020,dot_prod:98.452\n",
            "Epoch:1, Retrival step:276,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:98.586\n",
            "Epoch:1, Retrival step:277,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:98.694\n",
            "Epoch:1, Retrival step:278,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.300,reward:0.020,dot_prod:98.793\n",
            "Epoch:1, Retrival step:279,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.020,dot_prod:98.908\n",
            "Epoch:1, Retrival step:280,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:98.960\n",
            "Epoch:1, Retrival step:281,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:99.035\n",
            "Epoch:1, Retrival step:282,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.019,dot_prod:99.049\n",
            "Epoch:1, Retrival step:283,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.019,dot_prod:99.060\n",
            "Epoch:1, Retrival step:284,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:99.085\n",
            "Epoch:1, Retrival step:285,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.019,dot_prod:99.154\n",
            "Epoch:1, Retrival step:286,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.019,dot_prod:99.195\n",
            "Epoch:1, Retrival step:287,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.019,dot_prod:99.241\n",
            "Epoch:1, Retrival step:288,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.019,dot_prod:99.338\n",
            "Epoch:1, Retrival step:289,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.020,dot_prod:99.449\n",
            "Epoch:1, Retrival step:290,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:99.554\n",
            "Epoch:1, Retrival step:291,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.020,dot_prod:99.590\n",
            "Epoch:1, Retrival step:292,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.019,dot_prod:99.602\n",
            "Epoch:1, Retrival step:293,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.019,dot_prod:99.578\n",
            "Epoch:1, Retrival step:294,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:99.742\n",
            "Epoch:1, Retrival step:295,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.293,reward:0.019,dot_prod:99.875\n",
            "Epoch:1, Retrival step:296,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.319,reward:0.019,dot_prod:99.957\n",
            "Epoch:1, Retrival step:297,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.019,dot_prod:99.989\n",
            "Epoch:1, Retrival step:298,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.259,reward:0.019,dot_prod:100.103\n",
            "Epoch:1, Retrival step:299,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.019,dot_prod:100.114\n",
            "Generation step:300,loss:4.821,ppl:205.316,ret_time:0.081,gen_time:0.386\n",
            "Epoch:1, Retrival step:300,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.330,reward:0.019,dot_prod:100.197\n",
            "Epoch:1, Retrival step:301,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.303,reward:0.019,dot_prod:100.212\n",
            "Epoch:1, Retrival step:302,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.334,reward:0.019,dot_prod:100.312\n",
            "Epoch:1, Retrival step:303,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:100.376\n",
            "Epoch:1, Retrival step:304,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.303,reward:0.019,dot_prod:100.402\n",
            "Epoch:1, Retrival step:305,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.019,dot_prod:100.467\n",
            "Epoch:1, Retrival step:306,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.019,dot_prod:100.501\n",
            "Epoch:1, Retrival step:307,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.254,reward:0.019,dot_prod:100.549\n",
            "Epoch:1, Retrival step:308,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.019,dot_prod:100.541\n",
            "Epoch:1, Retrival step:309,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.019,dot_prod:100.525\n",
            "Epoch:1, Retrival step:310,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.310,reward:0.019,dot_prod:100.575\n",
            "Epoch:1, Retrival step:311,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.019,dot_prod:100.546\n",
            "Epoch:1, Retrival step:312,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.019,dot_prod:100.563\n",
            "Epoch:1, Retrival step:313,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.019,dot_prod:100.584\n",
            "Epoch:1, Retrival step:314,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.019,dot_prod:100.595\n",
            "Epoch:1, Retrival step:315,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:100.564\n",
            "Epoch:1, Retrival step:316,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.287,reward:0.019,dot_prod:100.549\n",
            "Epoch:1, Retrival step:317,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.019,dot_prod:100.519\n",
            "Epoch:1, Retrival step:318,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.019,dot_prod:100.411\n",
            "Epoch:1, Retrival step:319,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.019,dot_prod:100.402\n",
            "Epoch:1, Retrival step:320,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.019,dot_prod:100.393\n",
            "Epoch:1, Retrival step:321,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.302,reward:0.019,dot_prod:100.330\n",
            "Epoch:1, Retrival step:322,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:100.296\n",
            "Epoch:1, Retrival step:323,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.019,dot_prod:100.254\n",
            "Epoch:1, Retrival step:324,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:100.192\n",
            "Epoch:1, Retrival step:325,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.019,dot_prod:100.145\n",
            "Epoch:1, Retrival step:326,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.303,reward:0.019,dot_prod:100.085\n",
            "Epoch:1, Retrival step:327,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.316,reward:0.019,dot_prod:100.017\n",
            "Epoch:1, Retrival step:328,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.287,reward:0.019,dot_prod:100.004\n",
            "Epoch:1, Retrival step:329,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.019,dot_prod:99.988\n",
            "Epoch:1, Retrival step:330,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.278,reward:0.019,dot_prod:99.870\n",
            "Epoch:1, Retrival step:331,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.019,dot_prod:99.784\n",
            "Epoch:1, Retrival step:332,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.258,reward:0.019,dot_prod:99.701\n",
            "Epoch:1, Retrival step:333,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.019,dot_prod:99.626\n",
            "Epoch:1, Retrival step:334,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.313,reward:0.019,dot_prod:99.540\n",
            "Epoch:1, Retrival step:335,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.019,dot_prod:99.476\n",
            "Epoch:1, Retrival step:336,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.286,reward:0.019,dot_prod:99.413\n",
            "Epoch:1, Retrival step:337,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:99.387\n",
            "Epoch:1, Retrival step:338,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:99.336\n",
            "Epoch:1, Retrival step:339,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:99.274\n",
            "Epoch:1, Retrival step:340,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.293,reward:0.019,dot_prod:99.277\n",
            "Epoch:1, Retrival step:341,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.282,reward:0.019,dot_prod:99.169\n",
            "Epoch:1, Retrival step:342,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:99.149\n",
            "Epoch:1, Retrival step:343,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.280,reward:0.019,dot_prod:99.066\n",
            "Epoch:1, Retrival step:344,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:99.057\n",
            "Epoch:1, Retrival step:345,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:98.979\n",
            "Epoch:1, Retrival step:346,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.270,reward:0.019,dot_prod:98.870\n",
            "Epoch:1, Retrival step:347,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:98.802\n",
            "Epoch:1, Retrival step:348,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:98.757\n",
            "Epoch:1, Retrival step:349,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:98.662\n",
            "Epoch:1, Retrival step:350,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.285,reward:0.019,dot_prod:98.566\n",
            "Epoch:1, Retrival step:351,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:98.535\n",
            "Epoch:1, Retrival step:352,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.019,dot_prod:98.414\n",
            "Epoch:1, Retrival step:353,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.315,reward:0.019,dot_prod:98.333\n",
            "Epoch:1, Retrival step:354,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.261,reward:0.019,dot_prod:98.236\n",
            "Epoch:1, Retrival step:355,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:98.181\n",
            "Epoch:1, Retrival step:356,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.019,dot_prod:98.122\n",
            "Epoch:1, Retrival step:357,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:98.037\n",
            "Epoch:1, Retrival step:358,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:97.937\n",
            "Epoch:1, Retrival step:359,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:97.867\n",
            "Epoch:1, Retrival step:360,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.326,reward:0.019,dot_prod:97.749\n",
            "Epoch:1, Retrival step:361,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:97.640\n",
            "Epoch:1, Retrival step:362,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.262,reward:0.019,dot_prod:97.572\n",
            "Epoch:1, Retrival step:363,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.019,dot_prod:97.529\n",
            "Epoch:1, Retrival step:364,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:97.460\n",
            "Epoch:1, Retrival step:365,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.290,reward:0.019,dot_prod:97.394\n",
            "Epoch:1, Retrival step:366,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.019,dot_prod:97.301\n",
            "Epoch:1, Retrival step:367,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:97.185\n",
            "Epoch:1, Retrival step:368,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.329,reward:0.019,dot_prod:97.118\n",
            "Epoch:1, Retrival step:369,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.313,reward:0.019,dot_prod:97.006\n",
            "Epoch:1, Retrival step:370,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.329,reward:0.019,dot_prod:96.906\n",
            "Epoch:1, Retrival step:371,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.291,reward:0.019,dot_prod:96.839\n",
            "Epoch:1, Retrival step:372,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.330,reward:0.019,dot_prod:96.739\n",
            "Epoch:1, Retrival step:373,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.340,reward:0.019,dot_prod:96.676\n",
            "Epoch:1, Retrival step:374,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.319,reward:0.019,dot_prod:96.576\n",
            "Epoch:1, Retrival step:375,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.309,reward:0.019,dot_prod:96.510\n",
            "Epoch:1, Retrival step:376,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.271,reward:0.019,dot_prod:96.458\n",
            "Epoch:1, Retrival step:377,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.326,reward:0.019,dot_prod:96.378\n",
            "Epoch:1, Retrival step:378,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.019,dot_prod:96.306\n",
            "Epoch:1, Retrival step:379,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:96.288\n",
            "Epoch:1, Retrival step:380,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.279,reward:0.019,dot_prod:96.247\n",
            "Epoch:1, Retrival step:381,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.019,dot_prod:96.175\n",
            "Epoch:1, Retrival step:382,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:96.129\n",
            "Epoch:1, Retrival step:383,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:96.066\n",
            "Epoch:1, Retrival step:384,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.266,reward:0.019,dot_prod:96.003\n",
            "Epoch:1, Retrival step:385,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.320,reward:0.019,dot_prod:95.933\n",
            "Epoch:1, Retrival step:386,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.278,reward:0.019,dot_prod:95.870\n",
            "Epoch:1, Retrival step:387,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.019,dot_prod:95.750\n",
            "Epoch:1, Retrival step:388,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.019,dot_prod:95.681\n",
            "Epoch:1, Retrival step:389,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:95.648\n",
            "Epoch:1, Retrival step:390,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.019,dot_prod:95.617\n",
            "Epoch:1, Retrival step:391,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:95.557\n",
            "Epoch:1, Retrival step:392,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.272,reward:0.019,dot_prod:95.488\n",
            "Epoch:1, Retrival step:393,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.287,reward:0.019,dot_prod:95.415\n",
            "Epoch:1, Retrival step:394,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.303,reward:0.019,dot_prod:95.283\n",
            "Epoch:1, Retrival step:395,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:95.207\n",
            "Epoch:1, Retrival step:396,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.019,dot_prod:95.135\n",
            "Epoch:1, Retrival step:397,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.019,dot_prod:95.075\n",
            "Epoch:1, Retrival step:398,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:95.006\n",
            "Epoch:1, Retrival step:399,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:94.967\n",
            "Generation step:400,loss:4.754,ppl:188.804,ret_time:0.081,gen_time:0.374\n",
            "Epoch:1, Retrival step:400,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:94.911\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.396 Val ppl 103.384 Val reward 0.016\n",
            "r step:401,eval_loss:4.39602898144722,eval_ppl:103.3835311794281,eval_reward:0.015965843211393802\n",
            "current learning rate: 1.5644567947554964e-05\n",
            "Epoch:1, Retrival step:401,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:94.845\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.383 Val ppl 102.397 Val reward 0.016\n",
            "g step:401,eval_loss:4.383038674235344,eval_ppl:102.39721001529693,eval_reward:0.0162158558673691\n",
            "current learning rate: 1.5625e-05\n",
            "Epoch:1, Retrival step:402,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.315,reward:0.019,dot_prod:94.736\n",
            "Epoch:1, Retrival step:403,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.322,reward:0.019,dot_prod:94.710\n",
            "Epoch:1, Retrival step:404,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:94.643\n",
            "Epoch:1, Retrival step:405,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:94.511\n",
            "Epoch:1, Retrival step:406,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.019,dot_prod:94.443\n",
            "Epoch:1, Retrival step:407,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:94.398\n",
            "Epoch:1, Retrival step:408,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.019,dot_prod:94.355\n",
            "Epoch:1, Retrival step:409,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.019,dot_prod:94.326\n",
            "Epoch:1, Retrival step:410,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:94.282\n",
            "Epoch:1, Retrival step:411,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.347,reward:0.019,dot_prod:94.255\n",
            "Epoch:1, Retrival step:412,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:94.173\n",
            "Epoch:1, Retrival step:413,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:94.151\n",
            "Epoch:1, Retrival step:414,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:94.131\n",
            "Epoch:1, Retrival step:415,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:94.075\n",
            "Epoch:1, Retrival step:416,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.315,reward:0.019,dot_prod:94.017\n",
            "Epoch:1, Retrival step:417,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.972\n",
            "Epoch:1, Retrival step:418,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.284,reward:0.019,dot_prod:93.955\n",
            "Epoch:1, Retrival step:419,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.019,dot_prod:93.874\n",
            "Epoch:1, Retrival step:420,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.019,dot_prod:93.854\n",
            "Epoch:1, Retrival step:421,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:93.834\n",
            "Epoch:1, Retrival step:422,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.019,dot_prod:93.820\n",
            "Epoch:1, Retrival step:423,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.800\n",
            "Epoch:1, Retrival step:424,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:93.789\n",
            "Epoch:1, Retrival step:425,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.318,reward:0.019,dot_prod:93.730\n",
            "Epoch:1, Retrival step:426,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.019,dot_prod:93.673\n",
            "Epoch:1, Retrival step:427,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.291,reward:0.019,dot_prod:93.635\n",
            "Epoch:1, Retrival step:428,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.271,reward:0.019,dot_prod:93.628\n",
            "Epoch:1, Retrival step:429,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:93.573\n",
            "Epoch:1, Retrival step:430,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.486\n",
            "Epoch:1, Retrival step:431,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.019,dot_prod:93.487\n",
            "Epoch:1, Retrival step:432,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.286,reward:0.019,dot_prod:93.466\n",
            "Epoch:1, Retrival step:433,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.338,reward:0.019,dot_prod:93.391\n",
            "Epoch:1, Retrival step:434,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.019,dot_prod:93.333\n",
            "Epoch:1, Retrival step:435,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.315,reward:0.019,dot_prod:93.285\n",
            "Epoch:1, Retrival step:436,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.293,reward:0.019,dot_prod:93.246\n",
            "Epoch:1, Retrival step:437,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:93.165\n",
            "Epoch:1, Retrival step:438,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.019,dot_prod:93.127\n",
            "Epoch:1, Retrival step:439,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:93.082\n",
            "Epoch:1, Retrival step:440,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.034\n",
            "Epoch:1, Retrival step:441,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:93.017\n",
            "Epoch:1, Retrival step:442,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.337,reward:0.019,dot_prod:93.053\n",
            "Epoch:1, Retrival step:443,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:93.046\n",
            "Epoch:1, Retrival step:444,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:92.994\n",
            "Epoch:1, Retrival step:445,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.292,reward:0.019,dot_prod:92.900\n",
            "Epoch:1, Retrival step:446,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:92.866\n",
            "Epoch:1, Retrival step:447,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:92.860\n",
            "Epoch:1, Retrival step:448,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:92.811\n",
            "Epoch:1, Retrival step:449,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.276,reward:0.019,dot_prod:92.730\n",
            "Epoch:1, Retrival step:450,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:92.734\n",
            "Epoch:1, Retrival step:451,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:92.666\n",
            "Epoch:1, Retrival step:452,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.019,dot_prod:92.621\n",
            "Epoch:1, Retrival step:453,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.305,reward:0.019,dot_prod:92.565\n",
            "Epoch:1, Retrival step:454,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.286,reward:0.019,dot_prod:92.530\n",
            "Epoch:1, Retrival step:455,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.273,reward:0.019,dot_prod:92.491\n",
            "Epoch:1, Retrival step:456,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.316,reward:0.019,dot_prod:92.503\n",
            "Epoch:1, Retrival step:457,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.296,reward:0.019,dot_prod:92.517\n",
            "Epoch:1, Retrival step:458,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:92.512\n",
            "Epoch:1, Retrival step:459,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.019,dot_prod:92.529\n",
            "Epoch:1, Retrival step:460,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.317,reward:0.019,dot_prod:92.556\n",
            "Epoch:1, Retrival step:461,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.019,dot_prod:92.595\n",
            "Epoch:1, Retrival step:462,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.320,reward:0.019,dot_prod:92.595\n",
            "Epoch:1, Retrival step:463,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:92.540\n",
            "Epoch:1, Retrival step:464,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.335,reward:0.019,dot_prod:92.588\n",
            "Epoch:1, Retrival step:465,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:92.598\n",
            "Epoch:1, Retrival step:466,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.337,reward:0.019,dot_prod:92.612\n",
            "Epoch:1, Retrival step:467,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.019,dot_prod:92.649\n",
            "Epoch:1, Retrival step:468,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.284,reward:0.019,dot_prod:92.666\n",
            "Epoch:1, Retrival step:469,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:92.644\n",
            "Epoch:1, Retrival step:470,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:92.678\n",
            "Epoch:1, Retrival step:471,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.315,reward:0.019,dot_prod:92.701\n",
            "Epoch:1, Retrival step:472,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:92.768\n",
            "Epoch:1, Retrival step:473,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.769\n",
            "Epoch:1, Retrival step:474,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:92.808\n",
            "Epoch:1, Retrival step:475,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:92.835\n",
            "Epoch:1, Retrival step:476,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:92.871\n",
            "Epoch:1, Retrival step:477,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:92.861\n",
            "Epoch:1, Retrival step:478,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:92.838\n",
            "Epoch:1, Retrival step:479,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.292,reward:0.019,dot_prod:92.846\n",
            "Epoch:1, Retrival step:480,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.305,reward:0.019,dot_prod:92.848\n",
            "Epoch:1, Retrival step:481,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:92.878\n",
            "Epoch:1, Retrival step:482,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:92.908\n",
            "Epoch:1, Retrival step:483,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:92.920\n",
            "Epoch:1, Retrival step:484,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.955\n",
            "Epoch:1, Retrival step:485,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.268,reward:0.019,dot_prod:92.937\n",
            "Epoch:1, Retrival step:486,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.295,reward:0.019,dot_prod:92.946\n",
            "Epoch:1, Retrival step:487,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.919\n",
            "Epoch:1, Retrival step:488,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.295,reward:0.019,dot_prod:92.923\n",
            "Epoch:1, Retrival step:489,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.325,reward:0.019,dot_prod:92.938\n",
            "Epoch:1, Retrival step:490,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:92.966\n",
            "Epoch:1, Retrival step:491,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.311,reward:0.019,dot_prod:93.006\n",
            "Epoch:1, Retrival step:492,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.019,dot_prod:93.024\n",
            "Epoch:1, Retrival step:493,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.270,reward:0.019,dot_prod:92.990\n",
            "Epoch:1, Retrival step:494,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:92.962\n",
            "Epoch:1, Retrival step:495,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:92.971\n",
            "Epoch:1, Retrival step:496,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.019,dot_prod:92.972\n",
            "Epoch:1, Retrival step:497,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:92.986\n",
            "Epoch:1, Retrival step:498,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.019,dot_prod:92.999\n",
            "Epoch:1, Retrival step:499,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:92.962\n",
            "Generation step:500,loss:4.635,ppl:167.837,ret_time:0.081,gen_time:0.456\n",
            "Epoch:1, Retrival step:500,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.349,reward:0.019,dot_prod:92.951\n",
            "Epoch:1, Retrival step:501,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:92.967\n",
            "Epoch:1, Retrival step:502,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.327,reward:0.019,dot_prod:92.966\n",
            "Epoch:1, Retrival step:503,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.019,dot_prod:92.973\n",
            "Epoch:1, Retrival step:504,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:92.976\n",
            "Epoch:1, Retrival step:505,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.019,dot_prod:92.992\n",
            "Epoch:1, Retrival step:506,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.299,reward:0.019,dot_prod:92.979\n",
            "Epoch:1, Retrival step:507,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.294,reward:0.019,dot_prod:92.956\n",
            "Epoch:1, Retrival step:508,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:92.940\n",
            "Epoch:1, Retrival step:509,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:92.972\n",
            "Epoch:1, Retrival step:510,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:92.958\n",
            "Epoch:1, Retrival step:511,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:92.953\n",
            "Epoch:1, Retrival step:512,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.974\n",
            "Epoch:1, Retrival step:513,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.019,dot_prod:92.968\n",
            "Epoch:1, Retrival step:514,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:92.953\n",
            "Epoch:1, Retrival step:515,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:92.911\n",
            "Epoch:1, Retrival step:516,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.019,dot_prod:92.909\n",
            "Epoch:1, Retrival step:517,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.019,dot_prod:92.895\n",
            "Epoch:1, Retrival step:518,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.328,reward:0.019,dot_prod:92.866\n",
            "Epoch:1, Retrival step:519,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.285,reward:0.019,dot_prod:92.843\n",
            "Epoch:1, Retrival step:520,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.294,reward:0.019,dot_prod:92.833\n",
            "Epoch:1, Retrival step:521,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.317,reward:0.019,dot_prod:92.842\n",
            "Epoch:1, Retrival step:522,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.281,reward:0.019,dot_prod:92.816\n",
            "Epoch:1, Retrival step:523,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.019,dot_prod:92.834\n",
            "Epoch:1, Retrival step:524,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:92.813\n",
            "Epoch:1, Retrival step:525,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.316,reward:0.019,dot_prod:92.817\n",
            "Epoch:1, Retrival step:526,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:92.814\n",
            "Epoch:1, Retrival step:527,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.327,reward:0.019,dot_prod:92.818\n",
            "Epoch:1, Retrival step:528,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.304,reward:0.019,dot_prod:92.831\n",
            "Epoch:1, Retrival step:529,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:92.801\n",
            "Epoch:1, Retrival step:530,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:92.809\n",
            "Epoch:1, Retrival step:531,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:92.816\n",
            "Epoch:1, Retrival step:532,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:92.807\n",
            "Epoch:1, Retrival step:533,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:92.808\n",
            "Epoch:1, Retrival step:534,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.019,dot_prod:92.790\n",
            "Epoch:1, Retrival step:535,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.328,reward:0.019,dot_prod:92.804\n",
            "Epoch:1, Retrival step:536,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:92.796\n",
            "Epoch:1, Retrival step:537,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:92.817\n",
            "Epoch:1, Retrival step:538,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.286,reward:0.019,dot_prod:92.765\n",
            "Epoch:1, Retrival step:539,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.019,dot_prod:92.747\n",
            "Epoch:1, Retrival step:540,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.287,reward:0.019,dot_prod:92.756\n",
            "Epoch:1, Retrival step:541,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:92.780\n",
            "Epoch:1, Retrival step:542,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.267,reward:0.019,dot_prod:92.737\n",
            "Epoch:1, Retrival step:543,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:92.702\n",
            "Epoch:1, Retrival step:544,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:92.715\n",
            "Epoch:1, Retrival step:545,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.275,reward:0.019,dot_prod:92.725\n",
            "Epoch:1, Retrival step:546,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.308,reward:0.019,dot_prod:92.757\n",
            "Epoch:1, Retrival step:547,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.318,reward:0.019,dot_prod:92.773\n",
            "Epoch:1, Retrival step:548,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:92.793\n",
            "Epoch:1, Retrival step:549,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:92.814\n",
            "Epoch:1, Retrival step:550,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.019,dot_prod:92.812\n",
            "Epoch:1, Retrival step:551,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:92.861\n",
            "Epoch:1, Retrival step:552,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.289,reward:0.019,dot_prod:92.889\n",
            "Epoch:1, Retrival step:553,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.019,dot_prod:92.850\n",
            "Epoch:1, Retrival step:554,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:92.885\n",
            "Epoch:1, Retrival step:555,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:92.861\n",
            "Epoch:1, Retrival step:556,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.286,reward:0.019,dot_prod:92.871\n",
            "Epoch:1, Retrival step:557,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:92.858\n",
            "Epoch:1, Retrival step:558,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.019,dot_prod:92.868\n",
            "Epoch:1, Retrival step:559,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.296,reward:0.019,dot_prod:92.887\n",
            "Epoch:1, Retrival step:560,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.314,reward:0.019,dot_prod:92.890\n",
            "Epoch:1, Retrival step:561,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.019,dot_prod:92.887\n",
            "Epoch:1, Retrival step:562,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:92.880\n",
            "Epoch:1, Retrival step:563,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.314,reward:0.019,dot_prod:92.900\n",
            "Epoch:1, Retrival step:564,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:92.880\n",
            "Epoch:1, Retrival step:565,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:92.898\n",
            "Epoch:1, Retrival step:566,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:92.909\n",
            "Epoch:1, Retrival step:567,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.274,reward:0.019,dot_prod:92.930\n",
            "Epoch:1, Retrival step:568,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.270,reward:0.019,dot_prod:92.970\n",
            "Epoch:1, Retrival step:569,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.017\n",
            "Epoch:1, Retrival step:570,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.019,dot_prod:93.009\n",
            "Epoch:1, Retrival step:571,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:93.038\n",
            "Epoch:1, Retrival step:572,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.273,reward:0.019,dot_prod:93.023\n",
            "Epoch:1, Retrival step:573,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.327,reward:0.019,dot_prod:93.015\n",
            "Epoch:1, Retrival step:574,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.019,dot_prod:93.005\n",
            "Epoch:1, Retrival step:575,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.303,reward:0.019,dot_prod:93.019\n",
            "Epoch:1, Retrival step:576,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:93.019\n",
            "Epoch:1, Retrival step:577,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.305,reward:0.019,dot_prod:93.028\n",
            "Epoch:1, Retrival step:578,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.321,reward:0.019,dot_prod:93.030\n",
            "Epoch:1, Retrival step:579,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:93.045\n",
            "Epoch:1, Retrival step:580,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.316,reward:0.019,dot_prod:93.054\n",
            "Epoch:1, Retrival step:581,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.297,reward:0.019,dot_prod:93.082\n",
            "Epoch:1, Retrival step:582,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.019,dot_prod:93.059\n",
            "Epoch:1, Retrival step:583,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:93.051\n",
            "Epoch:1, Retrival step:584,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.019,dot_prod:93.061\n",
            "Epoch:1, Retrival step:585,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:93.089\n",
            "Epoch:1, Retrival step:586,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.019,dot_prod:93.045\n",
            "Epoch:1, Retrival step:587,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.319,reward:0.019,dot_prod:93.053\n",
            "Epoch:1, Retrival step:588,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.019,dot_prod:93.068\n",
            "Epoch:1, Retrival step:589,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.336,reward:0.019,dot_prod:93.106\n",
            "Epoch:1, Retrival step:590,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.320,reward:0.019,dot_prod:93.124\n",
            "Epoch:1, Retrival step:591,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:93.177\n",
            "Epoch:1, Retrival step:592,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.278,reward:0.019,dot_prod:93.192\n",
            "Epoch:1, Retrival step:593,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.313,reward:0.019,dot_prod:93.229\n",
            "Epoch:1, Retrival step:594,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.019,dot_prod:93.253\n",
            "Epoch:1, Retrival step:595,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.019,dot_prod:93.231\n",
            "Epoch:1, Retrival step:596,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:93.223\n",
            "Epoch:1, Retrival step:597,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.314,reward:0.019,dot_prod:93.274\n",
            "Epoch:1, Retrival step:598,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.286,reward:0.019,dot_prod:93.293\n",
            "Epoch:1, Retrival step:599,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.329,reward:0.019,dot_prod:93.293\n",
            "Generation step:600,loss:4.569,ppl:156.759,ret_time:0.082,gen_time:0.307\n",
            "Epoch:1, Retrival step:600,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:93.334\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.316 Val ppl 95.169 Val reward 0.017\n",
            "r step:601,eval_loss:4.3160937347412105,eval_ppl:95.16869075918197,eval_reward:0.01713465608865954\n",
            "current learning rate: 1.276840385070995e-05\n",
            "Epoch:1, Retrival step:601,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:93.364\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.309 Val ppl 94.761 Val reward 0.017\n",
            "g step:601,eval_loss:4.308807435750961,eval_ppl:94.76126360368728,eval_reward:0.017332327129784973\n",
            "current learning rate: 1.275775907699572e-05\n",
            "Epoch:1, Retrival step:602,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.019,dot_prod:93.377\n",
            "Epoch:1, Retrival step:603,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.322,reward:0.019,dot_prod:93.387\n",
            "Epoch:1, Retrival step:604,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.019,dot_prod:93.382\n",
            "Epoch:1, Retrival step:605,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.283,reward:0.019,dot_prod:93.393\n",
            "Epoch:1, Retrival step:606,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.019,dot_prod:93.416\n",
            "Epoch:1, Retrival step:607,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:93.461\n",
            "Epoch:1, Retrival step:608,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.019,dot_prod:93.460\n",
            "Epoch:1, Retrival step:609,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:93.463\n",
            "Epoch:1, Retrival step:610,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.264,reward:0.019,dot_prod:93.484\n",
            "Epoch:1, Retrival step:611,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:93.503\n",
            "Epoch:1, Retrival step:612,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.272,reward:0.019,dot_prod:93.527\n",
            "Epoch:1, Retrival step:613,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.019,dot_prod:93.537\n",
            "Epoch:1, Retrival step:614,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.264,reward:0.019,dot_prod:93.546\n",
            "Epoch:1, Retrival step:615,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.282,reward:0.019,dot_prod:93.564\n",
            "Epoch:1, Retrival step:616,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:93.572\n",
            "Epoch:1, Retrival step:617,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.019,dot_prod:93.557\n",
            "Epoch:1, Retrival step:618,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.019,dot_prod:93.551\n",
            "Epoch:1, Retrival step:619,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.336,reward:0.019,dot_prod:93.585\n",
            "Epoch:1, Retrival step:620,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.245,reward:0.019,dot_prod:93.572\n",
            "Epoch:1, Retrival step:621,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.297,reward:0.019,dot_prod:93.582\n",
            "Epoch:1, Retrival step:622,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:93.555\n",
            "Epoch:1, Retrival step:623,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.310,reward:0.019,dot_prod:93.590\n",
            "Epoch:1, Retrival step:624,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.339,reward:0.019,dot_prod:93.615\n",
            "Epoch:1, Retrival step:625,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.634\n",
            "Epoch:1, Retrival step:626,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.019,dot_prod:93.628\n",
            "Epoch:1, Retrival step:627,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.309,reward:0.019,dot_prod:93.643\n",
            "Epoch:1, Retrival step:628,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.339,reward:0.019,dot_prod:93.627\n",
            "Epoch:1, Retrival step:629,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:93.654\n",
            "Epoch:1, Retrival step:630,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:93.670\n",
            "Epoch:1, Retrival step:631,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:93.704\n",
            "Epoch:1, Retrival step:632,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.290,reward:0.019,dot_prod:93.698\n",
            "Epoch:1, Retrival step:633,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.261,reward:0.019,dot_prod:93.721\n",
            "Epoch:1, Retrival step:634,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.274,reward:0.019,dot_prod:93.694\n",
            "Epoch:1, Retrival step:635,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:93.748\n",
            "Epoch:1, Retrival step:636,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.019,dot_prod:93.779\n",
            "Epoch:1, Retrival step:637,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.296,reward:0.019,dot_prod:93.793\n",
            "Epoch:1, Retrival step:638,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:93.772\n",
            "Epoch:1, Retrival step:639,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:93.789\n",
            "Epoch:1, Retrival step:640,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:93.840\n",
            "Epoch:1, Retrival step:641,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.262,reward:0.019,dot_prod:93.839\n",
            "Epoch:1, Retrival step:642,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:93.849\n",
            "Epoch:1, Retrival step:643,loss:-0.000,ppl:0.000,ret_time:0.091,gen_time:0.280,reward:0.019,dot_prod:93.835\n",
            "Epoch:1, Retrival step:644,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.019,dot_prod:93.855\n",
            "Epoch:1, Retrival step:645,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.282,reward:0.019,dot_prod:93.861\n",
            "Epoch:1, Retrival step:646,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:93.863\n",
            "Epoch:1, Retrival step:647,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:93.923\n",
            "Epoch:1, Retrival step:648,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.019,dot_prod:93.942\n",
            "Epoch:1, Retrival step:649,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:93.981\n",
            "Epoch:1, Retrival step:650,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:94.008\n",
            "Epoch:1, Retrival step:651,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.330,reward:0.019,dot_prod:94.033\n",
            "Epoch:1, Retrival step:652,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.255,reward:0.019,dot_prod:94.012\n",
            "Epoch:1, Retrival step:653,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.314,reward:0.019,dot_prod:94.046\n",
            "Epoch:1, Retrival step:654,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.298,reward:0.019,dot_prod:94.000\n",
            "Epoch:1, Retrival step:655,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:93.998\n",
            "Epoch:1, Retrival step:656,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:94.006\n",
            "Epoch:1, Retrival step:657,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.019,dot_prod:94.024\n",
            "Epoch:1, Retrival step:658,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.281,reward:0.019,dot_prod:94.032\n",
            "Epoch:1, Retrival step:659,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.255,reward:0.019,dot_prod:94.038\n",
            "Epoch:1, Retrival step:660,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.019,dot_prod:94.038\n",
            "Epoch:1, Retrival step:661,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.264,reward:0.019,dot_prod:94.039\n",
            "Epoch:1, Retrival step:662,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:94.095\n",
            "Epoch:1, Retrival step:663,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:94.124\n",
            "Epoch:1, Retrival step:664,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.019,dot_prod:94.153\n",
            "Epoch:1, Retrival step:665,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.019,dot_prod:94.185\n",
            "Epoch:1, Retrival step:666,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.019,dot_prod:94.204\n",
            "Epoch:1, Retrival step:667,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.268,reward:0.019,dot_prod:94.217\n",
            "Epoch:1, Retrival step:668,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.280,reward:0.019,dot_prod:94.231\n",
            "Epoch:1, Retrival step:669,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.279,reward:0.019,dot_prod:94.245\n",
            "Epoch:1, Retrival step:670,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.318,reward:0.019,dot_prod:94.269\n",
            "Epoch:1, Retrival step:671,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.307,reward:0.019,dot_prod:94.265\n",
            "Epoch:1, Retrival step:672,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.019,dot_prod:94.285\n",
            "Epoch:1, Retrival step:673,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.282,reward:0.019,dot_prod:94.327\n",
            "Epoch:1, Retrival step:674,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.318,reward:0.019,dot_prod:94.340\n",
            "Epoch:1, Retrival step:675,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.326,reward:0.019,dot_prod:94.328\n",
            "Epoch:1, Retrival step:676,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:94.318\n",
            "Epoch:1, Retrival step:677,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.318,reward:0.019,dot_prod:94.326\n",
            "Epoch:1, Retrival step:678,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.290,reward:0.019,dot_prod:94.333\n",
            "Epoch:1, Retrival step:679,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.266,reward:0.019,dot_prod:94.347\n",
            "Epoch:1, Retrival step:680,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.300,reward:0.019,dot_prod:94.330\n",
            "Epoch:1, Retrival step:681,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.019,dot_prod:94.327\n",
            "Epoch:1, Retrival step:682,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:94.311\n",
            "Epoch:1, Retrival step:683,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.019,dot_prod:94.300\n",
            "Epoch:1, Retrival step:684,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:94.304\n",
            "Epoch:1, Retrival step:685,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.293,reward:0.019,dot_prod:94.293\n",
            "Epoch:1, Retrival step:686,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:94.305\n",
            "Epoch:1, Retrival step:687,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.320,reward:0.019,dot_prod:94.301\n",
            "Epoch:1, Retrival step:688,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:94.271\n",
            "Epoch:1, Retrival step:689,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.019,dot_prod:94.239\n",
            "Epoch:1, Retrival step:690,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.019,dot_prod:94.227\n",
            "Epoch:1, Retrival step:691,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.341,reward:0.019,dot_prod:94.215\n",
            "Epoch:1, Retrival step:692,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.279,reward:0.019,dot_prod:94.191\n",
            "Epoch:1, Retrival step:693,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:94.176\n",
            "Epoch:1, Retrival step:694,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.298,reward:0.019,dot_prod:94.157\n",
            "Epoch:1, Retrival step:695,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.019,dot_prod:94.135\n",
            "Epoch:1, Retrival step:696,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.019,dot_prod:94.137\n",
            "Epoch:1, Retrival step:697,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:94.111\n",
            "Epoch:1, Retrival step:698,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.339,reward:0.019,dot_prod:94.100\n",
            "Epoch:1, Retrival step:699,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.254,reward:0.019,dot_prod:94.061\n",
            "Generation step:700,loss:4.533,ppl:150.996,ret_time:0.081,gen_time:0.310\n",
            "Epoch:1, Retrival step:700,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:94.046\n",
            "Epoch:1, Retrival step:701,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.294,reward:0.019,dot_prod:94.040\n",
            "Epoch:1, Retrival step:702,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.326,reward:0.019,dot_prod:93.999\n",
            "Epoch:1, Retrival step:703,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:93.972\n",
            "Epoch:1, Retrival step:704,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.298,reward:0.019,dot_prod:93.954\n",
            "Epoch:1, Retrival step:705,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.316,reward:0.019,dot_prod:93.935\n",
            "Epoch:1, Retrival step:706,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.019,dot_prod:93.920\n",
            "Epoch:1, Retrival step:707,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:93.905\n",
            "Epoch:1, Retrival step:708,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.318,reward:0.019,dot_prod:93.889\n",
            "Epoch:1, Retrival step:709,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.288,reward:0.019,dot_prod:93.894\n",
            "Epoch:1, Retrival step:710,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.327,reward:0.019,dot_prod:93.882\n",
            "Epoch:1, Retrival step:711,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.339,reward:0.019,dot_prod:93.825\n",
            "Epoch:1, Retrival step:712,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.019,dot_prod:93.796\n",
            "Epoch:1, Retrival step:713,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.019,dot_prod:93.784\n",
            "Epoch:1, Retrival step:714,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.019,dot_prod:93.766\n",
            "Epoch:1, Retrival step:715,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:93.737\n",
            "Epoch:1, Retrival step:716,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:93.711\n",
            "Epoch:1, Retrival step:717,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:93.681\n",
            "Epoch:1, Retrival step:718,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:93.664\n",
            "Epoch:1, Retrival step:719,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.019,dot_prod:93.648\n",
            "Epoch:1, Retrival step:720,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.019,dot_prod:93.637\n",
            "Epoch:1, Retrival step:721,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.019,dot_prod:93.624\n",
            "Epoch:1, Retrival step:722,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.292,reward:0.019,dot_prod:93.608\n",
            "Epoch:1, Retrival step:723,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.019,dot_prod:93.601\n",
            "Epoch:1, Retrival step:724,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.307,reward:0.019,dot_prod:93.567\n",
            "Epoch:1, Retrival step:725,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:93.557\n",
            "Epoch:1, Retrival step:726,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:93.534\n",
            "Epoch:1, Retrival step:727,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:93.525\n",
            "Epoch:1, Retrival step:728,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.019,dot_prod:93.531\n",
            "Epoch:1, Retrival step:729,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.320,reward:0.019,dot_prod:93.518\n",
            "Epoch:1, Retrival step:730,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.289,reward:0.019,dot_prod:93.501\n",
            "Epoch:1, Retrival step:731,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:93.476\n",
            "Epoch:1, Retrival step:732,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:93.442\n",
            "Epoch:1, Retrival step:733,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:93.395\n",
            "Epoch:1, Retrival step:734,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.287,reward:0.019,dot_prod:93.370\n",
            "Epoch:1, Retrival step:735,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:93.350\n",
            "Epoch:1, Retrival step:736,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.263,reward:0.019,dot_prod:93.319\n",
            "Epoch:1, Retrival step:737,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:93.312\n",
            "Epoch:1, Retrival step:738,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.019,dot_prod:93.290\n",
            "Epoch:1, Retrival step:739,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.295,reward:0.019,dot_prod:93.310\n",
            "Epoch:1, Retrival step:740,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.286,reward:0.019,dot_prod:93.263\n",
            "Epoch:1, Retrival step:741,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.280,reward:0.019,dot_prod:93.246\n",
            "Epoch:1, Retrival step:742,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.306,reward:0.019,dot_prod:93.223\n",
            "Epoch:1, Retrival step:743,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.349,reward:0.019,dot_prod:93.214\n",
            "Epoch:1, Retrival step:744,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.019,dot_prod:93.214\n",
            "Epoch:1, Retrival step:745,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.319,reward:0.019,dot_prod:93.202\n",
            "Epoch:1, Retrival step:746,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.298,reward:0.019,dot_prod:93.178\n",
            "Epoch:1, Retrival step:747,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.318,reward:0.019,dot_prod:93.169\n",
            "Epoch:1, Retrival step:748,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.273,reward:0.019,dot_prod:93.151\n",
            "Epoch:1, Retrival step:749,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:93.143\n",
            "Epoch:1, Retrival step:750,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.019,dot_prod:93.129\n",
            "Epoch:1, Retrival step:751,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:93.073\n",
            "Epoch:1, Retrival step:752,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:93.089\n",
            "Epoch:1, Retrival step:753,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.317,reward:0.019,dot_prod:93.103\n",
            "Epoch:1, Retrival step:754,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.019,dot_prod:93.120\n",
            "Epoch:1, Retrival step:755,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:93.123\n",
            "Epoch:1, Retrival step:756,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:93.094\n",
            "Epoch:1, Retrival step:757,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:93.074\n",
            "Epoch:1, Retrival step:758,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:93.067\n",
            "Epoch:1, Retrival step:759,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.329,reward:0.019,dot_prod:93.061\n",
            "Epoch:1, Retrival step:760,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.303,reward:0.019,dot_prod:93.056\n",
            "Epoch:1, Retrival step:761,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.019,dot_prod:93.040\n",
            "Epoch:1, Retrival step:762,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:93.033\n",
            "Epoch:1, Retrival step:763,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.329,reward:0.019,dot_prod:93.035\n",
            "Epoch:1, Retrival step:764,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:93.028\n",
            "Epoch:1, Retrival step:765,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.268,reward:0.019,dot_prod:92.986\n",
            "Epoch:1, Retrival step:766,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.274,reward:0.019,dot_prod:92.996\n",
            "Epoch:1, Retrival step:767,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.019,dot_prod:93.001\n",
            "Epoch:1, Retrival step:768,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.262,reward:0.019,dot_prod:92.973\n",
            "Epoch:1, Retrival step:769,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.019,dot_prod:92.950\n",
            "Epoch:1, Retrival step:770,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:92.942\n",
            "Epoch:1, Retrival step:771,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.019,dot_prod:92.934\n",
            "Epoch:1, Retrival step:772,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.019,dot_prod:92.931\n",
            "Epoch:1, Retrival step:773,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.319,reward:0.019,dot_prod:92.942\n",
            "Epoch:1, Retrival step:774,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.019,dot_prod:92.958\n",
            "Epoch:1, Retrival step:775,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:92.952\n",
            "Epoch:1, Retrival step:776,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.923\n",
            "Epoch:1, Retrival step:777,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.284,reward:0.019,dot_prod:92.915\n",
            "Epoch:1, Retrival step:778,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.326,reward:0.019,dot_prod:92.909\n",
            "Epoch:1, Retrival step:779,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.019,dot_prod:92.913\n",
            "Epoch:1, Retrival step:780,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.019,dot_prod:92.895\n",
            "Epoch:1, Retrival step:781,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:92.891\n",
            "Epoch:1, Retrival step:782,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.336,reward:0.019,dot_prod:92.884\n",
            "Epoch:1, Retrival step:783,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.326,reward:0.019,dot_prod:92.884\n",
            "Epoch:1, Retrival step:784,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.328,reward:0.019,dot_prod:92.899\n",
            "Epoch:1, Retrival step:785,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.278,reward:0.019,dot_prod:92.896\n",
            "Epoch:1, Retrival step:786,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.339,reward:0.019,dot_prod:92.909\n",
            "Epoch:1, Retrival step:787,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.268,reward:0.019,dot_prod:92.892\n",
            "Epoch:1, Retrival step:788,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.019,dot_prod:92.895\n",
            "Epoch:1, Retrival step:789,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.019,dot_prod:92.894\n",
            "Epoch:1, Retrival step:790,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.276,reward:0.019,dot_prod:92.890\n",
            "Epoch:1, Retrival step:791,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.019,dot_prod:92.869\n",
            "Epoch:1, Retrival step:792,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.340,reward:0.019,dot_prod:92.851\n",
            "Epoch:1, Retrival step:793,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.298,reward:0.019,dot_prod:92.848\n",
            "Epoch:1, Retrival step:794,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:92.832\n",
            "Epoch:1, Retrival step:795,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.292,reward:0.019,dot_prod:92.839\n",
            "Epoch:1, Retrival step:796,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.019,dot_prod:92.814\n",
            "Epoch:1, Retrival step:797,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.290,reward:0.019,dot_prod:92.796\n",
            "Epoch:1, Retrival step:798,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.019,dot_prod:92.779\n",
            "Epoch:1, Retrival step:799,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.290,reward:0.019,dot_prod:92.765\n",
            "Generation step:800,loss:4.500,ppl:144.822,ret_time:0.082,gen_time:0.341\n",
            "Epoch:1, Retrival step:800,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.019,dot_prod:92.758\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.260 Val ppl 89.308 Val reward 0.018\n",
            "r step:801,eval_loss:4.26018445444107,eval_ppl:89.3082013502121,eval_reward:0.01799924893723801\n",
            "current learning rate: 1.105545527620664e-05\n",
            "Epoch:1, Retrival step:801,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.307,reward:0.019,dot_prod:92.736\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.252 Val ppl 88.752 Val reward 0.018\n",
            "g step:801,eval_loss:4.25173703789711,eval_ppl:88.75189474010467,eval_reward:0.0182220117226243\n",
            "current learning rate: 1.1048543456039805e-05\n",
            "Epoch:1, Retrival step:802,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.299,reward:0.019,dot_prod:92.712\n",
            "Epoch:1, Retrival step:803,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:92.697\n",
            "Epoch:1, Retrival step:804,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:92.676\n",
            "Epoch:1, Retrival step:805,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.019,dot_prod:92.657\n",
            "Epoch:1, Retrival step:806,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.326,reward:0.019,dot_prod:92.642\n",
            "Epoch:1, Retrival step:807,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.329,reward:0.019,dot_prod:92.615\n",
            "Epoch:1, Retrival step:808,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.323,reward:0.019,dot_prod:92.588\n",
            "Epoch:1, Retrival step:809,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:92.550\n",
            "Epoch:1, Retrival step:810,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.322,reward:0.019,dot_prod:92.533\n",
            "Epoch:1, Retrival step:811,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.284,reward:0.019,dot_prod:92.523\n",
            "Epoch:1, Retrival step:812,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.282,reward:0.019,dot_prod:92.500\n",
            "Epoch:1, Retrival step:813,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.019,dot_prod:92.483\n",
            "Epoch:1, Retrival step:814,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.318,reward:0.019,dot_prod:92.484\n",
            "Epoch:1, Retrival step:815,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.330,reward:0.019,dot_prod:92.471\n",
            "Epoch:1, Retrival step:816,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.272,reward:0.019,dot_prod:92.476\n",
            "Epoch:1, Retrival step:817,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:92.463\n",
            "Epoch:1, Retrival step:818,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:92.457\n",
            "Epoch:1, Retrival step:819,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:92.456\n",
            "Epoch:1, Retrival step:820,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:92.452\n",
            "Epoch:1, Retrival step:821,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:92.441\n",
            "Epoch:1, Retrival step:822,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:92.428\n",
            "Epoch:1, Retrival step:823,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.315,reward:0.019,dot_prod:92.399\n",
            "Epoch:1, Retrival step:824,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.305,reward:0.019,dot_prod:92.395\n",
            "Epoch:1, Retrival step:825,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:92.364\n",
            "Epoch:1, Retrival step:826,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:92.365\n",
            "Epoch:1, Retrival step:827,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:92.352\n",
            "Epoch:1, Retrival step:828,loss:-0.000,ppl:0.000,ret_time:0.091,gen_time:0.296,reward:0.019,dot_prod:92.329\n",
            "Epoch:1, Retrival step:829,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:92.339\n",
            "Epoch:1, Retrival step:830,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.019,dot_prod:92.346\n",
            "Epoch:1, Retrival step:831,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.276,reward:0.019,dot_prod:92.328\n",
            "Epoch:1, Retrival step:832,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.019,dot_prod:92.327\n",
            "Epoch:1, Retrival step:833,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.019,dot_prod:92.277\n",
            "Epoch:1, Retrival step:834,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.319,reward:0.019,dot_prod:92.282\n",
            "Epoch:1, Retrival step:835,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:92.274\n",
            "Epoch:1, Retrival step:836,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.019,dot_prod:92.260\n",
            "Epoch:1, Retrival step:837,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.019,dot_prod:92.257\n",
            "Epoch:1, Retrival step:838,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.328,reward:0.019,dot_prod:92.239\n",
            "Epoch:1, Retrival step:839,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.019,dot_prod:92.245\n",
            "Epoch:1, Retrival step:840,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.329,reward:0.019,dot_prod:92.247\n",
            "Epoch:1, Retrival step:841,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.330,reward:0.019,dot_prod:92.218\n",
            "Epoch:1, Retrival step:842,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:92.233\n",
            "Epoch:1, Retrival step:843,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.019,dot_prod:92.212\n",
            "Epoch:1, Retrival step:844,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.019,dot_prod:92.201\n",
            "Epoch:1, Retrival step:845,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.316,reward:0.019,dot_prod:92.183\n",
            "Epoch:1, Retrival step:846,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.019,dot_prod:92.169\n",
            "Epoch:1, Retrival step:847,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.019,dot_prod:92.155\n",
            "Epoch:1, Retrival step:848,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.327,reward:0.019,dot_prod:92.115\n",
            "Epoch:1, Retrival step:849,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.019,dot_prod:92.078\n",
            "Epoch:1, Retrival step:850,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.309,reward:0.019,dot_prod:92.090\n",
            "Epoch:1, Retrival step:851,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.285,reward:0.019,dot_prod:92.058\n",
            "Epoch:1, Retrival step:852,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.297,reward:0.019,dot_prod:92.061\n",
            "Epoch:1, Retrival step:853,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.307,reward:0.019,dot_prod:92.040\n",
            "Epoch:1, Retrival step:854,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:92.017\n",
            "Epoch:1, Retrival step:855,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.019,dot_prod:92.004\n",
            "Epoch:1, Retrival step:856,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.297,reward:0.019,dot_prod:91.996\n",
            "Epoch:1, Retrival step:857,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.019,dot_prod:91.964\n",
            "Epoch:1, Retrival step:858,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:91.948\n",
            "Epoch:1, Retrival step:859,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:91.922\n",
            "Epoch:1, Retrival step:860,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.275,reward:0.019,dot_prod:91.913\n",
            "Epoch:1, Retrival step:861,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.282,reward:0.019,dot_prod:91.898\n",
            "Epoch:1, Retrival step:862,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:91.895\n",
            "Epoch:1, Retrival step:863,loss:-0.000,ppl:0.000,ret_time:0.090,gen_time:0.311,reward:0.019,dot_prod:91.873\n",
            "Epoch:1, Retrival step:864,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.283,reward:0.019,dot_prod:91.858\n",
            "Epoch:1, Retrival step:865,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:91.838\n",
            "Epoch:1, Retrival step:866,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.019,dot_prod:91.825\n",
            "Epoch:1, Retrival step:867,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.020,dot_prod:91.804\n",
            "Epoch:1, Retrival step:868,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:91.796\n",
            "Epoch:1, Retrival step:869,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:91.731\n",
            "Epoch:1, Retrival step:870,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.020,dot_prod:91.694\n",
            "Epoch:1, Retrival step:871,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.338,reward:0.019,dot_prod:91.693\n",
            "Epoch:1, Retrival step:872,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:91.683\n",
            "Epoch:1, Retrival step:873,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.305,reward:0.020,dot_prod:91.684\n",
            "Epoch:1, Retrival step:874,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:91.652\n",
            "Epoch:1, Retrival step:875,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.294,reward:0.020,dot_prod:91.629\n",
            "Epoch:1, Retrival step:876,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.280,reward:0.020,dot_prod:91.604\n",
            "Epoch:1, Retrival step:877,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.020,dot_prod:91.594\n",
            "Epoch:1, Retrival step:878,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.275,reward:0.020,dot_prod:91.576\n",
            "Epoch:1, Retrival step:879,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.019,dot_prod:91.547\n",
            "Epoch:1, Retrival step:880,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.278,reward:0.019,dot_prod:91.520\n",
            "Epoch:1, Retrival step:881,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.019,dot_prod:91.499\n",
            "Epoch:1, Retrival step:882,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:91.476\n",
            "Epoch:1, Retrival step:883,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.019,dot_prod:91.446\n",
            "Epoch:1, Retrival step:884,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.019,dot_prod:91.433\n",
            "Epoch:1, Retrival step:885,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.272,reward:0.019,dot_prod:91.401\n",
            "Epoch:1, Retrival step:886,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.257,reward:0.019,dot_prod:91.379\n",
            "Epoch:1, Retrival step:887,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.019,dot_prod:91.340\n",
            "Epoch:1, Retrival step:888,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:91.323\n",
            "Epoch:1, Retrival step:889,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:91.314\n",
            "Epoch:1, Retrival step:890,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.019,dot_prod:91.306\n",
            "Epoch:1, Retrival step:891,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:91.266\n",
            "Epoch:1, Retrival step:892,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.019,dot_prod:91.258\n",
            "Epoch:1, Retrival step:893,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.019,dot_prod:91.207\n",
            "Epoch:1, Retrival step:894,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.019,dot_prod:91.202\n",
            "Epoch:1, Retrival step:895,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.020,dot_prod:91.163\n",
            "Epoch:1, Retrival step:896,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.335,reward:0.020,dot_prod:91.130\n",
            "Epoch:1, Retrival step:897,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.019,dot_prod:91.114\n",
            "Epoch:1, Retrival step:898,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:91.070\n",
            "Epoch:1, Retrival step:899,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.020,dot_prod:91.035\n",
            "Generation step:900,loss:4.456,ppl:137.473,ret_time:0.081,gen_time:0.368\n",
            "Epoch:1, Retrival step:900,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.020,dot_prod:91.003\n",
            "Epoch:1, Retrival step:901,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.020,dot_prod:90.980\n",
            "Epoch:1, Retrival step:902,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.020,dot_prod:90.955\n",
            "Epoch:1, Retrival step:903,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:90.969\n",
            "Epoch:1, Retrival step:904,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.312,reward:0.019,dot_prod:90.961\n",
            "Epoch:1, Retrival step:905,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.019,dot_prod:90.926\n",
            "Epoch:1, Retrival step:906,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.297,reward:0.019,dot_prod:90.909\n",
            "Epoch:1, Retrival step:907,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:90.876\n",
            "Epoch:1, Retrival step:908,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:90.877\n",
            "Epoch:1, Retrival step:909,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.019,dot_prod:90.849\n",
            "Epoch:1, Retrival step:910,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.282,reward:0.019,dot_prod:90.832\n",
            "Epoch:1, Retrival step:911,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.019,dot_prod:90.811\n",
            "Epoch:1, Retrival step:912,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.270,reward:0.019,dot_prod:90.774\n",
            "Epoch:1, Retrival step:913,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.253,reward:0.019,dot_prod:90.764\n",
            "Epoch:1, Retrival step:914,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:90.749\n",
            "Epoch:1, Retrival step:915,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.019,dot_prod:90.759\n",
            "Epoch:1, Retrival step:916,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.271,reward:0.019,dot_prod:90.728\n",
            "Epoch:1, Retrival step:917,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.289,reward:0.019,dot_prod:90.694\n",
            "Epoch:1, Retrival step:918,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:90.690\n",
            "Epoch:1, Retrival step:919,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.322,reward:0.019,dot_prod:90.666\n",
            "Epoch:1, Retrival step:920,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.307,reward:0.019,dot_prod:90.649\n",
            "Epoch:1, Retrival step:921,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.282,reward:0.019,dot_prod:90.640\n",
            "Epoch:1, Retrival step:922,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.285,reward:0.019,dot_prod:90.633\n",
            "Epoch:1, Retrival step:923,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:90.602\n",
            "Epoch:1, Retrival step:924,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:90.600\n",
            "Epoch:1, Retrival step:925,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.019,dot_prod:90.586\n",
            "Epoch:1, Retrival step:926,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.338,reward:0.019,dot_prod:90.581\n",
            "Epoch:1, Retrival step:927,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:90.577\n",
            "Epoch:1, Retrival step:928,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.019,dot_prod:90.555\n",
            "Epoch:1, Retrival step:929,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:90.535\n",
            "Epoch:1, Retrival step:930,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.019,dot_prod:90.532\n",
            "Epoch:1, Retrival step:931,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.277,reward:0.019,dot_prod:90.529\n",
            "Epoch:1, Retrival step:932,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.019,dot_prod:90.504\n",
            "Epoch:1, Retrival step:933,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.019,dot_prod:90.481\n",
            "Epoch:1, Retrival step:934,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.274,reward:0.019,dot_prod:90.473\n",
            "Epoch:1, Retrival step:935,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.279,reward:0.019,dot_prod:90.466\n",
            "Epoch:1, Retrival step:936,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.302,reward:0.019,dot_prod:90.447\n",
            "Epoch:1, Retrival step:937,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:90.411\n",
            "Epoch:1, Retrival step:938,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.259,reward:0.019,dot_prod:90.386\n",
            "Epoch:1, Retrival step:939,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.286,reward:0.019,dot_prod:90.373\n",
            "Epoch:1, Retrival step:940,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:90.351\n",
            "Epoch:1, Retrival step:941,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.019,dot_prod:90.359\n",
            "Epoch:1, Retrival step:942,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.019,dot_prod:90.361\n",
            "Epoch:1, Retrival step:943,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:90.322\n",
            "Epoch:1, Retrival step:944,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.019,dot_prod:90.306\n",
            "Epoch:1, Retrival step:945,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.019,dot_prod:90.308\n",
            "Epoch:1, Retrival step:946,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.267,reward:0.019,dot_prod:90.294\n",
            "Epoch:1, Retrival step:947,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.284,reward:0.019,dot_prod:90.280\n",
            "Epoch:1, Retrival step:948,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.313,reward:0.019,dot_prod:90.271\n",
            "Epoch:1, Retrival step:949,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.019,dot_prod:90.239\n",
            "Epoch:1, Retrival step:950,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:90.222\n",
            "Epoch:1, Retrival step:951,loss:-0.000,ppl:0.000,ret_time:0.079,gen_time:0.254,reward:0.019,dot_prod:90.220\n",
            "Epoch:1, Retrival step:952,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:90.195\n",
            "Epoch:1, Retrival step:953,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.019,dot_prod:90.189\n",
            "Epoch:1, Retrival step:954,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.019,dot_prod:90.201\n",
            "Epoch:1, Retrival step:955,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.019,dot_prod:90.202\n",
            "Epoch:1, Retrival step:956,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.019,dot_prod:90.187\n",
            "Epoch:1, Retrival step:957,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.019,dot_prod:90.168\n",
            "Epoch:1, Retrival step:958,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.019,dot_prod:90.184\n",
            "Epoch:1, Retrival step:959,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.019,dot_prod:90.183\n",
            "Epoch:1, Retrival step:960,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.019,dot_prod:90.159\n",
            "Epoch:1, Retrival step:961,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.310,reward:0.019,dot_prod:90.164\n",
            "Epoch:1, Retrival step:962,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.329,reward:0.019,dot_prod:90.166\n",
            "Epoch:1, Retrival step:963,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.019,dot_prod:90.139\n",
            "Epoch:1, Retrival step:964,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.266,reward:0.020,dot_prod:90.144\n",
            "Epoch:1, Retrival step:965,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.266,reward:0.020,dot_prod:90.154\n",
            "Epoch:1, Retrival step:966,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.020,dot_prod:90.157\n",
            "Epoch:1, Retrival step:967,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.276,reward:0.020,dot_prod:90.151\n",
            "Epoch:1, Retrival step:968,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.249,reward:0.019,dot_prod:90.153\n",
            "Epoch:1, Retrival step:969,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.019,dot_prod:90.141\n",
            "Epoch:1, Retrival step:970,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.020,dot_prod:90.152\n",
            "Epoch:1, Retrival step:971,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:90.141\n",
            "Epoch:1, Retrival step:972,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.302,reward:0.020,dot_prod:90.132\n",
            "Epoch:1, Retrival step:973,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:90.118\n",
            "Epoch:1, Retrival step:974,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.282,reward:0.020,dot_prod:90.109\n",
            "Epoch:1, Retrival step:975,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.020,dot_prod:90.102\n",
            "Epoch:1, Retrival step:976,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.261,reward:0.020,dot_prod:90.092\n",
            "Epoch:1, Retrival step:977,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.020,dot_prod:90.068\n",
            "Epoch:1, Retrival step:978,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.020,dot_prod:90.053\n",
            "Epoch:1, Retrival step:979,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.297,reward:0.020,dot_prod:90.057\n",
            "Epoch:1, Retrival step:980,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.294,reward:0.020,dot_prod:90.059\n",
            "Epoch:1, Retrival step:981,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.020,dot_prod:90.038\n",
            "Epoch:1, Retrival step:982,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:90.000\n",
            "Epoch:1, Retrival step:983,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:90.005\n",
            "Epoch:1, Retrival step:984,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.283,reward:0.020,dot_prod:89.999\n",
            "Epoch:1, Retrival step:985,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.997\n",
            "Epoch:1, Retrival step:986,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.996\n",
            "Epoch:1, Retrival step:987,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.020,dot_prod:89.992\n",
            "Epoch:1, Retrival step:988,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.308,reward:0.020,dot_prod:89.964\n",
            "Epoch:1, Retrival step:989,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.949\n",
            "Epoch:1, Retrival step:990,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.930\n",
            "Epoch:1, Retrival step:991,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:89.926\n",
            "Epoch:1, Retrival step:992,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.297,reward:0.020,dot_prod:89.934\n",
            "Epoch:1, Retrival step:993,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.917\n",
            "Epoch:1, Retrival step:994,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.283,reward:0.020,dot_prod:89.909\n",
            "Epoch:1, Retrival step:995,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.272,reward:0.020,dot_prod:89.921\n",
            "Epoch:1, Retrival step:996,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.317,reward:0.020,dot_prod:89.934\n",
            "Epoch:1, Retrival step:997,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.316,reward:0.020,dot_prod:89.927\n",
            "Epoch:1, Retrival step:998,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:89.924\n",
            "Epoch:1, Retrival step:999,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.020,dot_prod:89.900\n",
            "Generation step:1000,loss:4.436,ppl:134.261,ret_time:0.082,gen_time:0.298\n",
            "Epoch:1, Retrival step:1000,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.020,dot_prod:89.876\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.253 Val ppl 88.572 Val reward 0.018\n",
            "r step:1001,eval_loss:4.253392074346542,eval_ppl:88.57194633436202,eval_reward:0.018055846128845587\n",
            "current learning rate: 9.887062455755198e-06\n",
            "Epoch:1, Retrival step:1001,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.288,reward:0.020,dot_prod:89.863\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.245 Val ppl 88.108 Val reward 0.018\n",
            "g step:1001,eval_loss:4.245454452872276,eval_ppl:88.10786465454102,eval_reward:0.01832776042900514\n",
            "current learning rate: 9.882117688026186e-06\n",
            "Epoch:1, Retrival step:1002,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.020,dot_prod:89.859\n",
            "Epoch:1, Retrival step:1003,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:89.844\n",
            "Epoch:1, Retrival step:1004,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.279,reward:0.020,dot_prod:89.849\n",
            "Epoch:1, Retrival step:1005,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.020,dot_prod:89.846\n",
            "Epoch:1, Retrival step:1006,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.299,reward:0.020,dot_prod:89.823\n",
            "Epoch:1, Retrival step:1007,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.802\n",
            "Epoch:1, Retrival step:1008,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.278,reward:0.020,dot_prod:89.786\n",
            "Epoch:1, Retrival step:1009,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.020,dot_prod:89.771\n",
            "Epoch:1, Retrival step:1010,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.262,reward:0.020,dot_prod:89.758\n",
            "Epoch:1, Retrival step:1011,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.020,dot_prod:89.742\n",
            "Epoch:1, Retrival step:1012,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.330,reward:0.020,dot_prod:89.741\n",
            "Epoch:1, Retrival step:1013,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.727\n",
            "Epoch:1, Retrival step:1014,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.324,reward:0.020,dot_prod:89.715\n",
            "Epoch:1, Retrival step:1015,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1016,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.320,reward:0.020,dot_prod:89.724\n",
            "Epoch:1, Retrival step:1017,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.728\n",
            "Epoch:1, Retrival step:1018,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.020,dot_prod:89.704\n",
            "Epoch:1, Retrival step:1019,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.332,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1020,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.311,reward:0.020,dot_prod:89.713\n",
            "Epoch:1, Retrival step:1021,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.020,dot_prod:89.719\n",
            "Epoch:1, Retrival step:1022,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.020,dot_prod:89.706\n",
            "Epoch:1, Retrival step:1023,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.708\n",
            "Epoch:1, Retrival step:1024,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.284,reward:0.020,dot_prod:89.693\n",
            "Epoch:1, Retrival step:1025,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1026,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.327,reward:0.020,dot_prod:89.691\n",
            "Epoch:1, Retrival step:1027,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.316,reward:0.020,dot_prod:89.686\n",
            "Epoch:1, Retrival step:1028,loss:-0.000,ppl:0.000,ret_time:0.079,gen_time:0.256,reward:0.020,dot_prod:89.677\n",
            "Epoch:1, Retrival step:1029,loss:-0.000,ppl:0.000,ret_time:0.090,gen_time:0.288,reward:0.020,dot_prod:89.660\n",
            "Epoch:1, Retrival step:1030,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.280,reward:0.020,dot_prod:89.659\n",
            "Epoch:1, Retrival step:1031,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.323,reward:0.020,dot_prod:89.657\n",
            "Epoch:1, Retrival step:1032,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.310,reward:0.020,dot_prod:89.675\n",
            "Epoch:1, Retrival step:1033,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.020,dot_prod:89.680\n",
            "Epoch:1, Retrival step:1034,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.305,reward:0.020,dot_prod:89.692\n",
            "Epoch:1, Retrival step:1035,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.283,reward:0.020,dot_prod:89.696\n",
            "Epoch:1, Retrival step:1036,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.296,reward:0.020,dot_prod:89.671\n",
            "Epoch:1, Retrival step:1037,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.324,reward:0.020,dot_prod:89.676\n",
            "Epoch:1, Retrival step:1038,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.288,reward:0.020,dot_prod:89.686\n",
            "Epoch:1, Retrival step:1039,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.287,reward:0.020,dot_prod:89.703\n",
            "Epoch:1, Retrival step:1040,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.313,reward:0.020,dot_prod:89.718\n",
            "Epoch:1, Retrival step:1041,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.306,reward:0.020,dot_prod:89.704\n",
            "Epoch:1, Retrival step:1042,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.326,reward:0.020,dot_prod:89.691\n",
            "Epoch:1, Retrival step:1043,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.307,reward:0.020,dot_prod:89.698\n",
            "Epoch:1, Retrival step:1044,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.020,dot_prod:89.711\n",
            "Epoch:1, Retrival step:1045,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.020,dot_prod:89.724\n",
            "Epoch:1, Retrival step:1046,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.020,dot_prod:89.733\n",
            "Epoch:1, Retrival step:1047,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.285,reward:0.020,dot_prod:89.724\n",
            "Epoch:1, Retrival step:1048,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.729\n",
            "Epoch:1, Retrival step:1049,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.326,reward:0.020,dot_prod:89.736\n",
            "Epoch:1, Retrival step:1050,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.336,reward:0.020,dot_prod:89.738\n",
            "Epoch:1, Retrival step:1051,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.020,dot_prod:89.744\n",
            "Epoch:1, Retrival step:1052,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.328,reward:0.020,dot_prod:89.747\n",
            "Epoch:1, Retrival step:1053,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.315,reward:0.020,dot_prod:89.750\n",
            "Epoch:1, Retrival step:1054,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.336,reward:0.020,dot_prod:89.724\n",
            "Epoch:1, Retrival step:1055,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.020,dot_prod:89.715\n",
            "Epoch:1, Retrival step:1056,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1057,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.020,dot_prod:89.712\n",
            "Epoch:1, Retrival step:1058,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.324,reward:0.020,dot_prod:89.701\n",
            "Epoch:1, Retrival step:1059,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.693\n",
            "Epoch:1, Retrival step:1060,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.694\n",
            "Epoch:1, Retrival step:1061,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.290,reward:0.020,dot_prod:89.687\n",
            "Epoch:1, Retrival step:1062,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.307,reward:0.020,dot_prod:89.671\n",
            "Epoch:1, Retrival step:1063,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.292,reward:0.020,dot_prod:89.661\n",
            "Epoch:1, Retrival step:1064,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.305,reward:0.020,dot_prod:89.652\n",
            "Epoch:1, Retrival step:1065,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.326,reward:0.020,dot_prod:89.651\n",
            "Epoch:1, Retrival step:1066,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.304,reward:0.020,dot_prod:89.640\n",
            "Epoch:1, Retrival step:1067,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.320,reward:0.020,dot_prod:89.637\n",
            "Epoch:1, Retrival step:1068,loss:-0.000,ppl:0.000,ret_time:0.091,gen_time:0.325,reward:0.020,dot_prod:89.641\n",
            "Epoch:1, Retrival step:1069,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.316,reward:0.020,dot_prod:89.653\n",
            "Epoch:1, Retrival step:1070,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.308,reward:0.020,dot_prod:89.648\n",
            "Epoch:1, Retrival step:1071,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.020,dot_prod:89.647\n",
            "Epoch:1, Retrival step:1072,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.317,reward:0.020,dot_prod:89.636\n",
            "Epoch:1, Retrival step:1073,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.281,reward:0.020,dot_prod:89.652\n",
            "Epoch:1, Retrival step:1074,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.645\n",
            "Epoch:1, Retrival step:1075,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.020,dot_prod:89.634\n",
            "Epoch:1, Retrival step:1076,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.297,reward:0.020,dot_prod:89.635\n",
            "Epoch:1, Retrival step:1077,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.327,reward:0.020,dot_prod:89.652\n",
            "Epoch:1, Retrival step:1078,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.350,reward:0.020,dot_prod:89.652\n",
            "Epoch:1, Retrival step:1079,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.298,reward:0.020,dot_prod:89.655\n",
            "Epoch:1, Retrival step:1080,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.290,reward:0.020,dot_prod:89.653\n",
            "Epoch:1, Retrival step:1081,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.020,dot_prod:89.651\n",
            "Epoch:1, Retrival step:1082,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.020,dot_prod:89.658\n",
            "Epoch:1, Retrival step:1083,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.328,reward:0.020,dot_prod:89.669\n",
            "Epoch:1, Retrival step:1084,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.328,reward:0.020,dot_prod:89.671\n",
            "Epoch:1, Retrival step:1085,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.315,reward:0.020,dot_prod:89.665\n",
            "Epoch:1, Retrival step:1086,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.020,dot_prod:89.644\n",
            "Epoch:1, Retrival step:1087,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.308,reward:0.020,dot_prod:89.645\n",
            "Epoch:1, Retrival step:1088,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.322,reward:0.020,dot_prod:89.626\n",
            "Epoch:1, Retrival step:1089,loss:-0.000,ppl:0.000,ret_time:0.090,gen_time:0.312,reward:0.020,dot_prod:89.618\n",
            "Epoch:1, Retrival step:1090,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.329,reward:0.020,dot_prod:89.616\n",
            "Epoch:1, Retrival step:1091,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.327,reward:0.020,dot_prod:89.619\n",
            "Epoch:1, Retrival step:1092,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.287,reward:0.020,dot_prod:89.615\n",
            "Epoch:1, Retrival step:1093,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.328,reward:0.020,dot_prod:89.619\n",
            "Epoch:1, Retrival step:1094,loss:-0.000,ppl:0.000,ret_time:0.090,gen_time:0.307,reward:0.020,dot_prod:89.611\n",
            "Epoch:1, Retrival step:1095,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.280,reward:0.020,dot_prod:89.609\n",
            "Epoch:1, Retrival step:1096,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.020,dot_prod:89.614\n",
            "Epoch:1, Retrival step:1097,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.610\n",
            "Epoch:1, Retrival step:1098,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.325,reward:0.020,dot_prod:89.596\n",
            "Epoch:1, Retrival step:1099,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.020,dot_prod:89.606\n",
            "Generation step:1100,loss:4.419,ppl:131.244,ret_time:0.087,gen_time:0.293\n",
            "Epoch:1, Retrival step:1100,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.020,dot_prod:89.605\n",
            "Epoch:1, Retrival step:1101,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.020,dot_prod:89.598\n",
            "Epoch:1, Retrival step:1102,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:89.582\n",
            "Epoch:1, Retrival step:1103,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.020,dot_prod:89.557\n",
            "Epoch:1, Retrival step:1104,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:89.550\n",
            "Epoch:1, Retrival step:1105,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.020,dot_prod:89.538\n",
            "Epoch:1, Retrival step:1106,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.020,dot_prod:89.541\n",
            "Epoch:1, Retrival step:1107,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.541\n",
            "Epoch:1, Retrival step:1108,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.298,reward:0.020,dot_prod:89.541\n",
            "Epoch:1, Retrival step:1109,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.537\n",
            "Epoch:1, Retrival step:1110,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.020,dot_prod:89.548\n",
            "Epoch:1, Retrival step:1111,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.286,reward:0.020,dot_prod:89.557\n",
            "Epoch:1, Retrival step:1112,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.020,dot_prod:89.550\n",
            "Epoch:1, Retrival step:1113,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.297,reward:0.020,dot_prod:89.532\n",
            "Epoch:1, Retrival step:1114,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.279,reward:0.020,dot_prod:89.519\n",
            "Epoch:1, Retrival step:1115,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.020,dot_prod:89.514\n",
            "Epoch:1, Retrival step:1116,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.020,dot_prod:89.507\n",
            "Epoch:1, Retrival step:1117,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.310,reward:0.020,dot_prod:89.505\n",
            "Epoch:1, Retrival step:1118,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.275,reward:0.020,dot_prod:89.487\n",
            "Epoch:1, Retrival step:1119,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.020,dot_prod:89.482\n",
            "Epoch:1, Retrival step:1120,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.020,dot_prod:89.475\n",
            "Epoch:1, Retrival step:1121,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.020,dot_prod:89.463\n",
            "Epoch:1, Retrival step:1122,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:89.466\n",
            "Epoch:1, Retrival step:1123,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.298,reward:0.020,dot_prod:89.466\n",
            "Epoch:1, Retrival step:1124,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.020,dot_prod:89.453\n",
            "Epoch:1, Retrival step:1125,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.439\n",
            "Epoch:1, Retrival step:1126,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.420\n",
            "Epoch:1, Retrival step:1127,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.020,dot_prod:89.407\n",
            "Epoch:1, Retrival step:1128,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.317,reward:0.020,dot_prod:89.409\n",
            "Epoch:1, Retrival step:1129,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.410\n",
            "Epoch:1, Retrival step:1130,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.336,reward:0.020,dot_prod:89.394\n",
            "Epoch:1, Retrival step:1131,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.020,dot_prod:89.381\n",
            "Epoch:1, Retrival step:1132,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.020,dot_prod:89.365\n",
            "Epoch:1, Retrival step:1133,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.334\n",
            "Epoch:1, Retrival step:1134,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.323,reward:0.020,dot_prod:89.337\n",
            "Epoch:1, Retrival step:1135,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.276,reward:0.020,dot_prod:89.344\n",
            "Epoch:1, Retrival step:1136,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.340\n",
            "Epoch:1, Retrival step:1137,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.020,dot_prod:89.345\n",
            "Epoch:1, Retrival step:1138,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:89.338\n",
            "Epoch:1, Retrival step:1139,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.316,reward:0.020,dot_prod:89.340\n",
            "Epoch:1, Retrival step:1140,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.280,reward:0.020,dot_prod:89.336\n",
            "Epoch:1, Retrival step:1141,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.285,reward:0.020,dot_prod:89.342\n",
            "Epoch:1, Retrival step:1142,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.020,dot_prod:89.337\n",
            "Epoch:1, Retrival step:1143,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.291,reward:0.020,dot_prod:89.326\n",
            "Epoch:1, Retrival step:1144,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.319,reward:0.020,dot_prod:89.323\n",
            "Epoch:1, Retrival step:1145,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.319\n",
            "Epoch:1, Retrival step:1146,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.299,reward:0.020,dot_prod:89.303\n",
            "Epoch:1, Retrival step:1147,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.020,dot_prod:89.299\n",
            "Epoch:1, Retrival step:1148,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.020,dot_prod:89.291\n",
            "Epoch:1, Retrival step:1149,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.289,reward:0.020,dot_prod:89.284\n",
            "Epoch:1, Retrival step:1150,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.323,reward:0.020,dot_prod:89.264\n",
            "Epoch:1, Retrival step:1151,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.265,reward:0.020,dot_prod:89.276\n",
            "Epoch:1, Retrival step:1152,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.315,reward:0.020,dot_prod:89.277\n",
            "Epoch:1, Retrival step:1153,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:89.288\n",
            "Epoch:1, Retrival step:1154,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.314,reward:0.020,dot_prod:89.285\n",
            "Epoch:1, Retrival step:1155,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.308,reward:0.020,dot_prod:89.277\n",
            "Epoch:1, Retrival step:1156,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.290,reward:0.020,dot_prod:89.282\n",
            "Epoch:1, Retrival step:1157,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.328,reward:0.020,dot_prod:89.288\n",
            "Epoch:1, Retrival step:1158,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.320,reward:0.020,dot_prod:89.285\n",
            "Epoch:1, Retrival step:1159,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.308,reward:0.020,dot_prod:89.293\n",
            "Epoch:1, Retrival step:1160,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.318,reward:0.020,dot_prod:89.278\n",
            "Epoch:1, Retrival step:1161,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.310,reward:0.020,dot_prod:89.274\n",
            "Epoch:1, Retrival step:1162,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:89.300\n",
            "Epoch:1, Retrival step:1163,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.340,reward:0.020,dot_prod:89.324\n",
            "Epoch:1, Retrival step:1164,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.308,reward:0.020,dot_prod:89.331\n",
            "Epoch:1, Retrival step:1165,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.020,dot_prod:89.352\n",
            "Epoch:1, Retrival step:1166,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.304,reward:0.020,dot_prod:89.363\n",
            "Epoch:1, Retrival step:1167,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.309,reward:0.020,dot_prod:89.343\n",
            "Epoch:1, Retrival step:1168,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.276,reward:0.020,dot_prod:89.368\n",
            "Epoch:1, Retrival step:1169,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.289,reward:0.020,dot_prod:89.384\n",
            "Epoch:1, Retrival step:1170,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.297,reward:0.020,dot_prod:89.395\n",
            "Epoch:1, Retrival step:1171,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:89.412\n",
            "Epoch:1, Retrival step:1172,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.340,reward:0.020,dot_prod:89.417\n",
            "Epoch:1, Retrival step:1173,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.287,reward:0.020,dot_prod:89.440\n",
            "Epoch:1, Retrival step:1174,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.300,reward:0.020,dot_prod:89.461\n",
            "Epoch:1, Retrival step:1175,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.468\n",
            "Epoch:1, Retrival step:1176,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.312,reward:0.020,dot_prod:89.488\n",
            "Epoch:1, Retrival step:1177,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.312,reward:0.020,dot_prod:89.505\n",
            "Epoch:1, Retrival step:1178,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.020,dot_prod:89.520\n",
            "Epoch:1, Retrival step:1179,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:89.540\n",
            "Epoch:1, Retrival step:1180,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.296,reward:0.020,dot_prod:89.545\n",
            "Epoch:1, Retrival step:1181,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.020,dot_prod:89.576\n",
            "Epoch:1, Retrival step:1182,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.020,dot_prod:89.575\n",
            "Epoch:1, Retrival step:1183,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.299,reward:0.020,dot_prod:89.558\n",
            "Epoch:1, Retrival step:1184,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.350,reward:0.020,dot_prod:89.580\n",
            "Epoch:1, Retrival step:1185,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.318,reward:0.020,dot_prod:89.604\n",
            "Epoch:1, Retrival step:1186,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.020,dot_prod:89.652\n",
            "Epoch:1, Retrival step:1187,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:89.670\n",
            "Epoch:1, Retrival step:1188,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:89.679\n",
            "Epoch:1, Retrival step:1189,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.694\n",
            "Epoch:1, Retrival step:1190,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.328,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1191,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.309,reward:0.020,dot_prod:89.734\n",
            "Epoch:1, Retrival step:1192,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.745\n",
            "Epoch:1, Retrival step:1193,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.284,reward:0.020,dot_prod:89.743\n",
            "Epoch:1, Retrival step:1194,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.280,reward:0.020,dot_prod:89.751\n",
            "Epoch:1, Retrival step:1195,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.297,reward:0.020,dot_prod:89.753\n",
            "Epoch:1, Retrival step:1196,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.297,reward:0.020,dot_prod:89.752\n",
            "Epoch:1, Retrival step:1197,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.295,reward:0.020,dot_prod:89.764\n",
            "Epoch:1, Retrival step:1198,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.330,reward:0.020,dot_prod:89.763\n",
            "Epoch:1, Retrival step:1199,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.271,reward:0.020,dot_prod:89.785\n",
            "Generation step:1200,loss:4.400,ppl:127.861,ret_time:0.081,gen_time:0.318\n",
            "Epoch:1, Retrival step:1200,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.807\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            " /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Reading saved model from /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            " **************** CONFIGURATION **************** \n",
            "adam_eps                       -->   1e-08\n",
            "avg_by_prob                    -->   False\n",
            "batch_size                     -->   8\n",
            "change_id_over_card            -->   True\n",
            "config                         -->   None\n",
            "continue_from                  -->   0\n",
            "ctx_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   True\n",
            "dropout                        -->   0.1\n",
            "encoded_ctx_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "encoder_model_type             -->   ance_roberta\n",
            "encoding                       -->   True\n",
            "eval_batch_size                -->   1\n",
            "eval_input_file                -->   data/arxiv_test.txt\n",
            "eval_on_each                   -->   False\n",
            "file_suffix                    -->   \n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "g_only                         -->   False\n",
            "gradient_accumulation_steps    -->   1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "init_checkpoint                -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_generator.pkl\n",
            "learning_rate                  -->   1e-06\n",
            "load_trained_model             -->   True\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "log_dir                        -->   None\n",
            "loss_scale                     -->   0\n",
            "lr_schedule                    -->   noam\n",
            "max_seq_length                 -->   512\n",
            "model_file                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/reddit_retriever.pkl\n",
            "model_name_or_path             -->   configs\n",
            "n_docs                         -->   2\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "no_token_id                    -->   False\n",
            "normalize_data                 -->   True\n",
            "num_optim_steps                -->   16000\n",
            "num_shards                     -->   100\n",
            "out_file                       -->   /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "output_dir                     -->   /content/drive/My Drive/Colab Notebooks/RetGen/outputs/joint\n",
            "pbar                           -->   False\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "print_step                     -->   100\n",
            "projection_dim                 -->   0\n",
            "r_only                         -->   False\n",
            "ret_correction                 -->   False\n",
            "retriever_master_rank          -->   False\n",
            "reverse                        -->   False\n",
            "rl_method                      -->   simple\n",
            "seed                           -->   42\n",
            "sequence_length                -->   512\n",
            "set_type_embedding_to_zero     -->   False\n",
            "shard_folder                   -->   False\n",
            "shard_id                       -->   0\n",
            "skip_eval                      -->   False\n",
            "train_batch_size               -->   1\n",
            "train_input_file               -->   /content/drive/My Drive/Colab Notebooks/RetGen/data/arxiv_train.512len.db\n",
            "valid_step                     -->   200\n",
            "validation_workers             -->   16\n",
            "warmup_proportion              -->   0.1\n",
            "warmup_steps                   -->   1\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "reading data from file=/content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Writing results to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total passages processed 57607. Written to /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "encoded files: /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_*\n",
            "Reading all passages data from files: ['/content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0']\n",
            "Reading file /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt_0\n",
            "Total data indexed 50000\n",
            "Total data indexed 57607\n",
            "Data indexing completed.\n",
            "Reading data from: /content/drive/My Drive/Colab Notebooks/RetGen/data/wiki.txt\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.223 Val ppl 86.021 Val reward 0.019\n",
            "r step:1201,eval_loss:4.223268842458725,eval_ppl:86.02138743638993,eval_reward:0.018679191250004806\n",
            "current learning rate: 9.02485909777981e-06\n",
            "Epoch:1, Retrival step:1201,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.323,reward:0.020,dot_prod:89.815\n",
            "compute eval model loss, using eval mode, please change it back to train after calling this function\n",
            "\n",
            " Epoch 0: Val loss 4.210 Val ppl 85.106 Val reward 0.019\n",
            "g step:1201,eval_loss:4.210057440042496,eval_ppl:85.10598138856888,eval_reward:0.018965224510058762\n",
            "current learning rate: 9.021097956087903e-06\n",
            "Epoch:1, Retrival step:1202,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.292,reward:0.020,dot_prod:89.834\n",
            "Epoch:1, Retrival step:1203,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.822\n",
            "Epoch:1, Retrival step:1204,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.280,reward:0.020,dot_prod:89.833\n",
            "Epoch:1, Retrival step:1205,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:89.846\n",
            "Epoch:1, Retrival step:1206,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.272,reward:0.020,dot_prod:89.857\n",
            "Epoch:1, Retrival step:1207,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.275,reward:0.020,dot_prod:89.858\n",
            "Epoch:1, Retrival step:1208,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:89.871\n",
            "Epoch:1, Retrival step:1209,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.269,reward:0.020,dot_prod:89.868\n",
            "Epoch:1, Retrival step:1210,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.336,reward:0.020,dot_prod:89.873\n",
            "Epoch:1, Retrival step:1211,loss:-0.000,ppl:0.000,ret_time:0.090,gen_time:0.325,reward:0.020,dot_prod:89.884\n",
            "Epoch:1, Retrival step:1212,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.272,reward:0.020,dot_prod:89.890\n",
            "Epoch:1, Retrival step:1213,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.893\n",
            "Epoch:1, Retrival step:1214,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.314,reward:0.020,dot_prod:89.908\n",
            "Epoch:1, Retrival step:1215,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.328,reward:0.020,dot_prod:89.914\n",
            "Epoch:1, Retrival step:1216,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.302,reward:0.020,dot_prod:89.914\n",
            "Epoch:1, Retrival step:1217,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.281,reward:0.020,dot_prod:89.899\n",
            "Epoch:1, Retrival step:1218,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.307,reward:0.020,dot_prod:89.902\n",
            "Epoch:1, Retrival step:1219,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.278,reward:0.020,dot_prod:89.915\n",
            "Epoch:1, Retrival step:1220,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.277,reward:0.020,dot_prod:89.916\n",
            "Epoch:1, Retrival step:1221,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.317,reward:0.020,dot_prod:89.930\n",
            "Epoch:1, Retrival step:1222,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.278,reward:0.020,dot_prod:89.928\n",
            "Epoch:1, Retrival step:1223,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.277,reward:0.020,dot_prod:89.920\n",
            "Epoch:1, Retrival step:1224,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.278,reward:0.020,dot_prod:89.921\n",
            "Epoch:1, Retrival step:1225,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.317,reward:0.020,dot_prod:89.927\n",
            "Epoch:1, Retrival step:1226,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.323,reward:0.020,dot_prod:89.917\n",
            "Epoch:1, Retrival step:1227,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.283,reward:0.020,dot_prod:89.911\n",
            "Epoch:1, Retrival step:1228,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.315,reward:0.020,dot_prod:89.909\n",
            "Epoch:1, Retrival step:1229,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.285,reward:0.020,dot_prod:89.904\n",
            "Epoch:1, Retrival step:1230,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.314,reward:0.020,dot_prod:89.910\n",
            "Epoch:1, Retrival step:1231,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.272,reward:0.020,dot_prod:89.907\n",
            "Epoch:1, Retrival step:1232,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.020,dot_prod:89.910\n",
            "Epoch:1, Retrival step:1233,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.304,reward:0.020,dot_prod:89.917\n",
            "Epoch:1, Retrival step:1234,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.311,reward:0.020,dot_prod:89.902\n",
            "Epoch:1, Retrival step:1235,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.874\n",
            "Epoch:1, Retrival step:1236,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.020,dot_prod:89.861\n",
            "Epoch:1, Retrival step:1237,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.309,reward:0.020,dot_prod:89.834\n",
            "Epoch:1, Retrival step:1238,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.293,reward:0.020,dot_prod:89.827\n",
            "Epoch:1, Retrival step:1239,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.311,reward:0.020,dot_prod:89.834\n",
            "Epoch:1, Retrival step:1240,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.827\n",
            "Epoch:1, Retrival step:1241,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.298,reward:0.020,dot_prod:89.830\n",
            "Epoch:1, Retrival step:1242,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.834\n",
            "Epoch:1, Retrival step:1243,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.286,reward:0.020,dot_prod:89.833\n",
            "Epoch:1, Retrival step:1244,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:89.822\n",
            "Epoch:1, Retrival step:1245,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:89.804\n",
            "Epoch:1, Retrival step:1246,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.795\n",
            "Epoch:1, Retrival step:1247,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.293,reward:0.020,dot_prod:89.787\n",
            "Epoch:1, Retrival step:1248,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.284,reward:0.020,dot_prod:89.797\n",
            "Epoch:1, Retrival step:1249,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.020,dot_prod:89.804\n",
            "Epoch:1, Retrival step:1250,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.325,reward:0.020,dot_prod:89.773\n",
            "Epoch:1, Retrival step:1251,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.020,dot_prod:89.755\n",
            "Epoch:1, Retrival step:1252,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.297,reward:0.020,dot_prod:89.743\n",
            "Epoch:1, Retrival step:1253,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.738\n",
            "Epoch:1, Retrival step:1254,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.302,reward:0.020,dot_prod:89.743\n",
            "Epoch:1, Retrival step:1255,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.020,dot_prod:89.747\n",
            "Epoch:1, Retrival step:1256,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.315,reward:0.020,dot_prod:89.743\n",
            "Epoch:1, Retrival step:1257,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.311,reward:0.020,dot_prod:89.768\n",
            "Epoch:1, Retrival step:1258,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.277,reward:0.020,dot_prod:89.783\n",
            "Epoch:1, Retrival step:1259,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.309,reward:0.020,dot_prod:89.777\n",
            "Epoch:1, Retrival step:1260,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.284,reward:0.020,dot_prod:89.757\n",
            "Epoch:1, Retrival step:1261,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.285,reward:0.020,dot_prod:89.735\n",
            "Epoch:1, Retrival step:1262,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.283,reward:0.020,dot_prod:89.727\n",
            "Epoch:1, Retrival step:1263,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.296,reward:0.020,dot_prod:89.731\n",
            "Epoch:1, Retrival step:1264,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:89.735\n",
            "Epoch:1, Retrival step:1265,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.305,reward:0.020,dot_prod:89.726\n",
            "Epoch:1, Retrival step:1266,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.717\n",
            "Epoch:1, Retrival step:1267,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.724\n",
            "Epoch:1, Retrival step:1268,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.335,reward:0.020,dot_prod:89.716\n",
            "Epoch:1, Retrival step:1269,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.323,reward:0.020,dot_prod:89.704\n",
            "Epoch:1, Retrival step:1270,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.348,reward:0.020,dot_prod:89.708\n",
            "Epoch:1, Retrival step:1271,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.296,reward:0.020,dot_prod:89.710\n",
            "Epoch:1, Retrival step:1272,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.305,reward:0.020,dot_prod:89.696\n",
            "Epoch:1, Retrival step:1273,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.304,reward:0.020,dot_prod:89.689\n",
            "Epoch:1, Retrival step:1274,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.323,reward:0.020,dot_prod:89.665\n",
            "Epoch:1, Retrival step:1275,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.654\n",
            "Epoch:1, Retrival step:1276,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.638\n",
            "Epoch:1, Retrival step:1277,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.304,reward:0.020,dot_prod:89.616\n",
            "Epoch:1, Retrival step:1278,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.272,reward:0.020,dot_prod:89.621\n",
            "Epoch:1, Retrival step:1279,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.283,reward:0.020,dot_prod:89.620\n",
            "Epoch:1, Retrival step:1280,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.020,dot_prod:89.616\n",
            "Epoch:1, Retrival step:1281,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.271,reward:0.020,dot_prod:89.623\n",
            "Epoch:1, Retrival step:1282,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:89.617\n",
            "Epoch:1, Retrival step:1283,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.276,reward:0.020,dot_prod:89.611\n",
            "Epoch:1, Retrival step:1284,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.282,reward:0.020,dot_prod:89.602\n",
            "Epoch:1, Retrival step:1285,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:89.588\n",
            "Epoch:1, Retrival step:1286,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:89.590\n",
            "Epoch:1, Retrival step:1287,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.296,reward:0.020,dot_prod:89.590\n",
            "Epoch:1, Retrival step:1288,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.020,dot_prod:89.588\n",
            "Epoch:1, Retrival step:1289,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.316,reward:0.020,dot_prod:89.575\n",
            "Epoch:1, Retrival step:1290,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.285,reward:0.020,dot_prod:89.575\n",
            "Epoch:1, Retrival step:1291,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:89.581\n",
            "Epoch:1, Retrival step:1292,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:89.570\n",
            "Epoch:1, Retrival step:1293,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.312,reward:0.020,dot_prod:89.560\n",
            "Epoch:1, Retrival step:1294,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.279,reward:0.020,dot_prod:89.572\n",
            "Epoch:1, Retrival step:1295,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.275,reward:0.020,dot_prod:89.549\n",
            "Epoch:1, Retrival step:1296,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.302,reward:0.020,dot_prod:89.530\n",
            "Epoch:1, Retrival step:1297,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.020,dot_prod:89.525\n",
            "Epoch:1, Retrival step:1298,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.282,reward:0.020,dot_prod:89.539\n",
            "Epoch:1, Retrival step:1299,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.540\n",
            "Generation step:1300,loss:4.384,ppl:125.993,ret_time:0.081,gen_time:0.224\n",
            "Epoch:1, Retrival step:1300,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.020,dot_prod:89.546\n",
            "Epoch:1, Retrival step:1301,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.287,reward:0.020,dot_prod:89.543\n",
            "Epoch:1, Retrival step:1302,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.540\n",
            "Epoch:1, Retrival step:1303,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.020,dot_prod:89.527\n",
            "Epoch:1, Retrival step:1304,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.282,reward:0.020,dot_prod:89.526\n",
            "Epoch:1, Retrival step:1305,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.523\n",
            "Epoch:1, Retrival step:1306,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.327,reward:0.020,dot_prod:89.522\n",
            "Epoch:1, Retrival step:1307,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:89.511\n",
            "Epoch:1, Retrival step:1308,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.520\n",
            "Epoch:1, Retrival step:1309,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.296,reward:0.020,dot_prod:89.521\n",
            "Epoch:1, Retrival step:1310,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.327,reward:0.020,dot_prod:89.502\n",
            "Epoch:1, Retrival step:1311,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.321,reward:0.020,dot_prod:89.491\n",
            "Epoch:1, Retrival step:1312,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.290,reward:0.020,dot_prod:89.502\n",
            "Epoch:1, Retrival step:1313,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.294,reward:0.020,dot_prod:89.505\n",
            "Epoch:1, Retrival step:1314,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.273,reward:0.020,dot_prod:89.497\n",
            "Epoch:1, Retrival step:1315,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.293,reward:0.020,dot_prod:89.495\n",
            "Epoch:1, Retrival step:1316,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.315,reward:0.020,dot_prod:89.489\n",
            "Epoch:1, Retrival step:1317,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.292,reward:0.020,dot_prod:89.480\n",
            "Epoch:1, Retrival step:1318,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.280,reward:0.020,dot_prod:89.473\n",
            "Epoch:1, Retrival step:1319,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.253,reward:0.020,dot_prod:89.456\n",
            "Epoch:1, Retrival step:1320,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.438\n",
            "Epoch:1, Retrival step:1321,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.315,reward:0.020,dot_prod:89.425\n",
            "Epoch:1, Retrival step:1322,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.322,reward:0.020,dot_prod:89.432\n",
            "Epoch:1, Retrival step:1323,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.438\n",
            "Epoch:1, Retrival step:1324,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.296,reward:0.020,dot_prod:89.414\n",
            "Epoch:1, Retrival step:1325,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.277,reward:0.020,dot_prod:89.407\n",
            "Epoch:1, Retrival step:1326,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:89.400\n",
            "Epoch:1, Retrival step:1327,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:89.392\n",
            "Epoch:1, Retrival step:1328,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.310,reward:0.020,dot_prod:89.408\n",
            "Epoch:1, Retrival step:1329,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.271,reward:0.020,dot_prod:89.398\n",
            "Epoch:1, Retrival step:1330,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.325,reward:0.020,dot_prod:89.386\n",
            "Epoch:1, Retrival step:1331,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.279,reward:0.020,dot_prod:89.376\n",
            "Epoch:1, Retrival step:1332,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:89.372\n",
            "Epoch:1, Retrival step:1333,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.326,reward:0.020,dot_prod:89.372\n",
            "Epoch:1, Retrival step:1334,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.313,reward:0.020,dot_prod:89.373\n",
            "Epoch:1, Retrival step:1335,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.370\n",
            "Epoch:1, Retrival step:1336,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.266,reward:0.020,dot_prod:89.342\n",
            "Epoch:1, Retrival step:1337,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.313,reward:0.020,dot_prod:89.350\n",
            "Epoch:1, Retrival step:1338,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.287,reward:0.020,dot_prod:89.344\n",
            "Epoch:1, Retrival step:1339,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.334\n",
            "Epoch:1, Retrival step:1340,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.325,reward:0.020,dot_prod:89.320\n",
            "Epoch:1, Retrival step:1341,loss:-0.000,ppl:0.000,ret_time:0.088,gen_time:0.304,reward:0.020,dot_prod:89.309\n",
            "Epoch:1, Retrival step:1342,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.316,reward:0.020,dot_prod:89.306\n",
            "Epoch:1, Retrival step:1343,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.324,reward:0.020,dot_prod:89.302\n",
            "Epoch:1, Retrival step:1344,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.303,reward:0.020,dot_prod:89.303\n",
            "Epoch:1, Retrival step:1345,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.283,reward:0.020,dot_prod:89.293\n",
            "Epoch:1, Retrival step:1346,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.324,reward:0.020,dot_prod:89.291\n",
            "Epoch:1, Retrival step:1347,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.284,reward:0.020,dot_prod:89.277\n",
            "Epoch:1, Retrival step:1348,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.309,reward:0.020,dot_prod:89.275\n",
            "Epoch:1, Retrival step:1349,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.265\n",
            "Epoch:1, Retrival step:1350,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.308,reward:0.020,dot_prod:89.247\n",
            "Epoch:1, Retrival step:1351,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.267,reward:0.020,dot_prod:89.252\n",
            "Epoch:1, Retrival step:1352,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.297,reward:0.020,dot_prod:89.238\n",
            "Epoch:1, Retrival step:1353,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.326,reward:0.020,dot_prod:89.241\n",
            "Epoch:1, Retrival step:1354,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.284,reward:0.020,dot_prod:89.229\n",
            "Epoch:1, Retrival step:1355,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.239\n",
            "Epoch:1, Retrival step:1356,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.318,reward:0.020,dot_prod:89.254\n",
            "Epoch:1, Retrival step:1357,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.294,reward:0.020,dot_prod:89.237\n",
            "Epoch:1, Retrival step:1358,loss:-0.000,ppl:0.000,ret_time:0.080,gen_time:0.280,reward:0.020,dot_prod:89.232\n",
            "Epoch:1, Retrival step:1359,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:89.208\n",
            "Epoch:1, Retrival step:1360,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.288,reward:0.020,dot_prod:89.201\n",
            "Epoch:1, Retrival step:1361,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.277,reward:0.020,dot_prod:89.203\n",
            "Epoch:1, Retrival step:1362,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.315,reward:0.020,dot_prod:89.208\n",
            "Epoch:1, Retrival step:1363,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.292,reward:0.020,dot_prod:89.198\n",
            "Epoch:1, Retrival step:1364,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.199\n",
            "Epoch:1, Retrival step:1365,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.327,reward:0.020,dot_prod:89.209\n",
            "Epoch:1, Retrival step:1366,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.248,reward:0.020,dot_prod:89.227\n",
            "Epoch:1, Retrival step:1367,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.325,reward:0.020,dot_prod:89.211\n",
            "Epoch:1, Retrival step:1368,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.292,reward:0.020,dot_prod:89.212\n",
            "Epoch:1, Retrival step:1369,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.309,reward:0.020,dot_prod:89.211\n",
            "Epoch:1, Retrival step:1370,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.302,reward:0.020,dot_prod:89.200\n",
            "Epoch:1, Retrival step:1371,loss:-0.000,ppl:0.000,ret_time:0.087,gen_time:0.293,reward:0.020,dot_prod:89.185\n",
            "Epoch:1, Retrival step:1372,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.281,reward:0.020,dot_prod:89.191\n",
            "Epoch:1, Retrival step:1373,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.311,reward:0.020,dot_prod:89.184\n",
            "Epoch:1, Retrival step:1374,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.295,reward:0.020,dot_prod:89.172\n",
            "Epoch:1, Retrival step:1375,loss:-0.000,ppl:0.000,ret_time:0.083,gen_time:0.319,reward:0.020,dot_prod:89.172\n",
            "Epoch:1, Retrival step:1376,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.315,reward:0.020,dot_prod:89.185\n",
            "Epoch:1, Retrival step:1377,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.306,reward:0.020,dot_prod:89.185\n",
            "Epoch:1, Retrival step:1378,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:89.192\n",
            "Epoch:1, Retrival step:1379,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.284,reward:0.020,dot_prod:89.180\n",
            "Epoch:1, Retrival step:1380,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.329,reward:0.020,dot_prod:89.170\n",
            "Epoch:1, Retrival step:1381,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.316,reward:0.020,dot_prod:89.181\n",
            "Epoch:1, Retrival step:1382,loss:-0.000,ppl:0.000,ret_time:0.079,gen_time:0.277,reward:0.020,dot_prod:89.179\n",
            "Epoch:1, Retrival step:1383,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.286,reward:0.020,dot_prod:89.167\n",
            "Epoch:1, Retrival step:1384,loss:-0.000,ppl:0.000,ret_time:0.084,gen_time:0.322,reward:0.020,dot_prod:89.152\n",
            "Epoch:1, Retrival step:1385,loss:-0.000,ppl:0.000,ret_time:0.086,gen_time:0.292,reward:0.020,dot_prod:89.146\n",
            "Epoch:1, Retrival step:1386,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.308,reward:0.020,dot_prod:89.139\n",
            "Epoch:1, Retrival step:1387,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.314,reward:0.020,dot_prod:89.131\n",
            "Epoch:1, Retrival step:1388,loss:-0.000,ppl:0.000,ret_time:0.079,gen_time:0.307,reward:0.020,dot_prod:89.132\n",
            "Epoch:1, Retrival step:1389,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.271,reward:0.020,dot_prod:89.140\n",
            "Epoch:1, Retrival step:1390,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.317,reward:0.020,dot_prod:89.131\n",
            "Epoch:1, Retrival step:1391,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.327,reward:0.020,dot_prod:89.123\n",
            "Epoch:1, Retrival step:1392,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:89.131\n",
            "Epoch:1, Retrival step:1393,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.271,reward:0.020,dot_prod:89.110\n",
            "Epoch:1, Retrival step:1394,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.278,reward:0.020,dot_prod:89.102\n",
            "Epoch:1, Retrival step:1395,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.315,reward:0.020,dot_prod:89.102\n",
            "Epoch:1, Retrival step:1396,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.287,reward:0.020,dot_prod:89.093\n",
            "Epoch:1, Retrival step:1397,loss:-0.000,ppl:0.000,ret_time:0.085,gen_time:0.324,reward:0.020,dot_prod:89.104\n",
            "Epoch:1, Retrival step:1398,loss:-0.000,ppl:0.000,ret_time:0.082,gen_time:0.314,reward:0.020,dot_prod:89.088\n",
            "Epoch:1, Retrival step:1399,loss:-0.000,ppl:0.000,ret_time:0.081,gen_time:0.312,reward:0.020,dot_prod:89.097\n",
            "Generation step:1400,loss:4.375,ppl:124.137,ret_time:0.084,gen_time:0.245\n",
            "Epoch:1, Retrival step:1400,loss:-0.000,ppl:0.000,ret_time:0.089,gen_time:0.278,reward:0.020,dot_prod:89.084\n",
            "Encoder vector_size=768\n",
            "Producing encodings to file(s): /content/drive/My Drive/Colab Notebooks/RetGen/models/dense_embedding/reddit_retriever.pkl.wiki.txt\n",
            "Producing encodings for passages range: 0 to 57607 (out of total 5760790)\n",
            "gen_ctx_vectors\n",
            "Traceback (most recent call last):\n",
            "  File \"joint_training.py\", line 469, in <module>\n",
            "    config, epoch, pbar, train_r_logger, eval_logger, output_dir, stats, experiment=experiment)\n",
            "  File \"/content/RetGen/retriever_ft.py\", line 74, in retriever_finetune\n",
            "    force_index=True, file_suffix = args.file_suffix)\n",
            "  File \"/content/RetGen/extract_top_docs.py\", line 186, in init_retriever\n",
            "    data = gen_ctx_vectors(rows, encoder, tensorizer, args, False)\n",
            "  File \"/content/RetGen/generate_dense_embeddings.py\", line 60, in gen_ctx_vectors\n",
            "    ctx_rows[batch_start:batch_start + bsz]]\n",
            "  File \"/content/RetGen/generate_dense_embeddings.py\", line 59, in <listcomp>\n",
            "    batch_token_tensors = [tensorizer.text_to_tensor(ctx[1], title=ctx[2] if insert_title else None) for ctx in\n",
            "  File \"/content/RetGen/dpr/models/hf_models.py\", line 256, in text_to_tensor\n",
            "    pad_to_max_length=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1434, in encode\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1576, in encode_plus\n",
            "    first_ids = get_input_ids(text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1548, in get_input_ids\n",
            "    tokens = self.tokenize(text, add_special_tokens=add_special_tokens, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1329, in tokenize\n",
            "    tokenized_text = split_on_tokens(added_tokens, text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1323, in split_on_tokens\n",
            "    for token in tokenized_text\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\", line 1323, in <genexpr>\n",
            "    for token in tokenized_text\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_bert.py\", line 210, in _tokenize\n",
            "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_bert.py\", line 380, in tokenize\n",
            "    text = self._clean_text(text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_bert.py\", line 477, in _clean_text\n",
            "    if _is_whitespace(char):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_bert.py\", line 544, in _is_whitespace\n",
            "    def _is_whitespace(char):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/RetGen\n",
        "!bash run.sh"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eCH8aKnGaa2e",
        "yQBxRTJ31CV_"
      ],
      "machine_shape": "hm",
      "name": "retgen_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}